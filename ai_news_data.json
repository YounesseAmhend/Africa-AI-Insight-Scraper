{
  "scrape_date": "2025-03-22 07:44:13",
  "total_articles": 28,
  "articles": [
    {
      "title": "Anthropic just gave Claude a superpower: real-time web search. Here’s why it changes everything",
      "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More\n\nAnthropic announced today that its AI assistant Claude can now search and process information from the internet in real-time, addressing one of users’ most requested features and closing a critical competitive gap with OpenAI’s ChatGPT.\n\nThe new web search capability, available immediately for paid Claude users in the United States, transforms the AI assistant from a tool limited by its training data cutoff to one that can access and synthesize the latest information across the web.\n\n“With web search, Claude has access to the latest events and information, boosting its accuracy on tasks that benefit from the most recent data,” Anthropic said in its announcement. The company emphasized that Claude will provide direct citations to sources, allowing users to fact-check information— a direct response to growing concerns about AI hallucinations and misinformation.\n\nAI arms race intensifies as Anthropic secures billions in funding\n\nThis launch comes at an important moment in the rapidly evolving AI sector. Just three weeks ago, Anthropic secured $3.5 billion in Series E funding at a post-money valuation of $61.5 billion, underscoring the high stakes in the AI race. Major backers include Lightspeed Venture Partners, Google (which holds a 14% stake) and Amazon, which has integrated Claude into its Alexa+ service.\n\nThe web search rollout also follows Anthropic’s recent release of Claude 3.7 Sonnet, which the company claims has set “a new high-water mark in coding abilities.” This focus on programming proficiency appears strategic, especially in light of CEO Dario Amodei’s recent prediction at a Council on Foreign Relations event that “in three to six months, AI will be writing 90% of the code” that software developers currently produce.\n\nThe timing of this feature launch reveals Anthropic’s determination to challenge OpenAI’s dominance in the consumer AI assistant market. While Claude has gained popularity among technical users for its nuanced reasoning and longer context window, the lack of real-time information access has been a significant handicap in head-to-head comparisons with ChatGPT. This update effectively neutralizes that disadvantage.\n\nHow Claude’s web search transforms enterprise decision-making\n\nUnlike traditional search engines that return a list of links, Claude processes search results and delivers them in a conversational format. Users simply toggle on web search in their profile settings, and Claude will automatically search the internet when needed to inform its responses.\n\nAnthropic highlighted several business use cases for the web-enabled Claude: sales teams analyzing industry trends, financial analysts assessing current market data, researchers building grant proposals and shoppers comparing products across multiple sources.\n\nThis feature fundamentally changes how enterprise users can interact with AI assistants. Previously, professionals needed to toggle between search engines and AI tools, manually feeding information from one to the other. Claude’s integrated approach streamlines this workflow dramatically, potentially saving hours of research time for knowledge workers.\n\nFor financial services firms in particular, the ability to combine historical training data with breaking news creates a powerful analysis tool that could provide genuine competitive advantages. Investment decisions often hinge on connecting disparate pieces of information quickly — exactly the kind of task this integration aims to solve.\n\nBehind the scenes: The technical infrastructure powering Claude’s new capabilities\n\nBehind this seemingly straightforward feature lies considerable technical complexity. Anthropic has likely spent months fine-tuning Claude’s ability to search effectively, understand context and determine when web search would improve its responses.\n\nThe update integrates with other recent technical improvements to the Anthropic API, including cache-aware rate limits, simpler prompt caching, and token-efficient tool use. These enhancements, announced earlier this month, aim to help developers process more requests while reducing costs. For certain applications, these enhancements can reduce token usage by up to 90%.\n\nAnthropic has also upgraded its developer console to enable collaboration among teams working on AI implementations. The revised console allows developers to share prompts, collaborate on refinements and control extended thinking budgets — features particularly valuable for enterprise customers integrating Claude into their workflows.\n\nThe investment in these backend capabilities suggests Anthropic is building for scale, anticipating rapid adoption as more companies integrate AI into their operations. By focusing on developer experience alongside user-facing features, Anthropic is creating an ecosystem rather than just a product — a strategy that has served companies like Microsoft well in enterprise markets.\n\nVoice mode: Anthropic’s next frontier in natural AI interaction\n\nA web search may be just the beginning of Anthropic’s feature expansion. According to a recent report in the Financial Times, the company is developing voice capabilities for Claude, potentially transforming how users interact with the AI assistant.\n\nMike Krieger, Anthropic’s chief product officer, told the Financial Times that the company is working on experiences that would allow users to speak directly to Claude. “We are doing some work around how Claude for desktop evolves… if it is going to be operating your computer, a more natural user interface might be to [speak to it],” Krieger said.\n\nThe company has reportedly held discussions with Amazon and voice-focused AI startup ElevenLabs about potential partnerships, though no deals have been finalized.\n\nVoice interaction would represent a significant leap forward in making AI assistants more accessible and intuitive. The current text-based interaction model creates friction that voice could eliminate, potentially expanding Claude’s appeal beyond tech-savvy early adopters to a much broader user base.\n\nHow Anthropic’s safety-first approach shapes regulatory conversations\n\nAs Anthropic expands Claude’s capabilities, the company continues to emphasize its commitment to responsible AI development. In response to California Governor Gavin Newsom’s Working Group on AI Frontier Models draft report released earlier this week, Anthropic expressed support for “objective standards and evidence-based policy guidance,” particularly highlighting transparency as “a low-cost, high-impact means of growing the evidence base around a new technology.”\n\n“Many of the report’s recommendations already reflect industry best practices which Anthropic adheres to,” the company stated, noting its Responsible Scaling Policy that outlines how it assesses models for misuse and autonomy risks.\n\nThis focus on responsible development represents a core differentiator in Anthropic’s brand positioning since its founding in 2021, when Amodei and six colleagues left OpenAI to create an AI company with greater emphasis on safety.\n\nAnthropic’s approach to regulation appears more collaborative than defensive, positioning the company favorably with policymakers who are increasingly focused on AI oversight. By proactively addressing safety concerns and contributing constructively to regulatory frameworks, Anthropic may be able to shape rules in ways that align with its existing practices while potentially creating compliance hurdles for less cautious competitors.\n\nThe future of AI assistants: From chatbots to indispensable digital partners\n\nAdding web search to Claude represents more than just feature parity with competitors — it signals Anthropic’s ambition to create AI systems that can function as comprehensive digital assistants rather than specialized tools.\n\nThis development marks a significant evolution in AI assistants. First-generation large language models were essentially sophisticated autocomplete systems with impressive but limited capabilities. The integration of real-time information access, combined with Claude’s existing reasoning abilities, creates something qualitatively different: a system that can actively help solve complex problems using up-to-date information.\n\nClaude’s new capabilities offer compelling advantages for businesses investing in AI integration. Cognition, the maker of the AI software developer assistant Devin, has already leveraged Anthropic’s prompt caching to provide more context about codebases while reducing costs and latency, according to the company’s leadership.\n\nThe real potential of these systems goes far beyond simple information retrieval. By combining current data with deep contextual understanding, AI assistants like Claude could transform knowledge work by handling substantial portions of research, analysis, and content creation — freeing humans to focus on judgment, creativity, and interpersonal aspects of their roles.\n\nWhat web search means for Claude users today and tomorrow\n\nWeb search is available now for all paid Claude users in the United States, with support for free users and international expansion planned “soon,” according to the announcement. Users can activate the feature through their profile settings.\n\nAs competition in the AI assistant space intensifies, Anthropic’s deliberate approach to expanding Claude’s capabilities while maintaining its focus on safety and transparency suggests a long-term strategy focused on building user trust alongside technical advancement.\n\nThe race between AI companies is increasingly about balancing capability with reliability and trust. Features like web search with source citations serve both goals simultaneously, providing users with more powerful tools while maintaining transparency about information sources.\n\nWith Claude now able to tap into the internet’s vast resources while maintaining its characteristic nuanced reasoning, Anthropic has eliminated a key competitive disadvantage. More importantly, the company has taken a significant step toward creating AI systems that don’t just respond to queries but actively help users navigate an increasingly complex information landscape.",
      "url": "https://venturebeat.com/ai/anthropic-just-gave-claude-a-superpower-real-time-web-search-heres-why-it-changes-everything/",
      "source": "https://venturebeat.com/category/ai/",
      "date": "2025-03-20",
      "authors": [
        "Michael Nuñez"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "Small models as paralegals: LexisNexis distills models to build AI assistant",
      "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More\n\nWhen legal research company LexisNexis created its AI assistant Protégé, it wanted to figure out the best way to leverage its expertise without deploying a large model.\n\nProtégé aims to help lawyers, associates and paralegals write and proof legal documents and ensure that anything they cite in complaints and briefs is accurate. However, LexisNexis didn’t want a general legal AI assistant; they wanted to build one that learns a firm’s workflow and is more customizable.\n\nLexisNexis saw the opportunity to bring the power of large language models (LLMs) from Anthropic and Mistral and find the best models that answer user questions the best, Jeff Reihl, CTO of LexisNexis Legal and Professional, told VentureBeat.\n\n“We use the best model for the specific use case as part of our multi-model approach. We use the model that provides the best result with the fastest response time,” Reihl said. “For some use cases, that will be a small language model like Mistral or we perform distillation to improve performance and reduce cost.”\n\nWhile LLMs still provide value in building AI applications, some organizations turn to using small language models (SLMs) or distilling LLMs to become small versions of the same model.\n\nDistillation, where an LLM “teaches” a smaller model, has become a popular method for many organizations.\n\nSmall models often work best for apps like chatbots or simple code completion, which is what LexisNexis wanted to use for Protégé.\n\nThis is not the first time LexisNexis built AI applications, even before launching its legal research hub LexisNexis + AI in July 2024.\n\n“We have used a lot of AI in the past, which was more around natural language processing, some deep learning and machine learning,” Reihl said. “That really changed in November 2022 when ChatGPT was launched, because prior to that, a lot of the AI capabilities were kind of behind the scenes. But once ChatGPT came out, the generative capabilities, the conversational capabilities of it was very, very intriguing to us.”\n\nSmall, fine-tuned models and model routing\n\nReihl said LexisNexis uses different models from most of the major model providers when building its AI platforms. LexisNexis + AI used Claude models from Anthropic, OpenAI’s GPT models and a model from Mistral.\n\nThis multimodal approach helped break down each task users wanted to perform on the platform. To do this, LexisNexis had to architect its platform to switch between models.\n\n“We would break down whatever task was being performed into individual components, and then we would identify the best large language model to support that component. One example of that is we will use Mistral to assess the query that the user entered in,” Reihl said.\n\nFor Protégé, the company wanted faster response times and models more fine-tuned for legal use cases. So it turned to what Reihl calls “fine-tuned” versions of models, essentially smaller weight versions of LLMs or distilled models.\n\n“You don’t need GPT-4o to do the assessment of a query, so we use it for more sophisticated work, and we switch models out,” he said.\n\nWhen a user asks Protégé a question about a specific case, the first model it pings is a fine-tuned Mistral “for assessing the query, then determining what the purpose and intent of that query is” before switching to the model best suited to complete the task. Reihl said the next model could be an LLM that generates new queries for the search engine or another model that summarizes results.\n\nRight now, LexisNexis mostly relies on a fine-tuned Mistral model though Reihl said it used a fine-tuned version of Claude “when it first came out; we are not using it in the product today but in other ways.” LexisNexis is also interested in using other OpenAI models especially since the company came out with new reinforcement fine-tuning capabilities last year. LexisNexis is in the process of evaluating OpenAI’s reasoning models including o3 for its platforms.\n\nReihl added that it may also look at using Gemini models from Google.\n\nLexisNexis backs all of its AI platforms with its own knowledge graph to perform retrieval augmented generation (RAG) capabilities, especially as Protégé could help launch agentic processes later.\n\nThe AI legal suite\n\nEven before the advent of generative AI, LexisNexis tested the possibility of putting chatbots to work in the legal industry. In 2017, the company tested an AI assistant that would compete with IBM’s Watson-powered Ross and Protégé sits in the company’s LexisNexis + AI platform, which brings together the AI services of LexisNexis.\n\nProtégé helps law firms with tasks that paralegals or associates tend to do. It helps write legal briefs and complaints that are grounded in firms’ documents and data, suggest legal workflow next steps, suggest new prompts to refine searches, draft questions for depositions and discovery, link quotes in filings for accuracy, generate timelines and, of course, summarize complex legal documents.\n\n“We see Protégé as the initial step in personalization and agentic capabilities,” Reihl said. “Think about the different types of lawyers: M&A, litigators, real estate. It’s going to continue to get more and more personalized based on the specific task you do. Our vision is that every legal professional will have a personal assistant to help them do their job based on what they do, not what other lawyers do.”\n\nProtégé now competes against other legal research and technology platforms. Thomson Reuters customized OpenAI’s o1-mini-model for its CoCounsel legal assistant. Harvey, which raised $300 million from investors including LexisNexis, also has a legal AI assistant.",
      "url": "https://venturebeat.com/ai/small-models-as-paralegals-lexisnexis-distills-models-to-build-ai-assistant/",
      "source": "https://venturebeat.com/category/ai/",
      "date": "2025-03-20",
      "authors": [
        "Emilia David"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "startup"
    },
    {
      "title": "Less is more: UC Berkeley and Google unlock LLM potential through simple sampling",
      "text": "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More\n\nA new paper by researchers from Google Research and the University of California, Berkeley, demonstrates that a surprisingly simple test-time scaling approach can boost the reasoning abilities of large language models (LLMs). The key? Scaling up sampling-based search, a technique that relies on generating multiple responses and using the model itself to verify them.\n\nThe core finding is that even a minimalist implementation of sampling-based search, using random sampling and self-verification, can elevate the reasoning performance of models like Gemini 1.5 Pro beyond that of o1-Preview on popular benchmarks. The findings can have important implications for enterprise applications and challenge the assumption that highly specialized training or complex architectures are always necessary for achieving top-tier performance.\n\nThe limits of current test-time compute scaling\n\nThe current popular method for test-time scaling in LLMs is to train the model through reinforcement learning to generate longer responses with chain-of-thought (CoT) traces. This approach is used in models such as OpenAI o1 and DeepSeek-R1. While beneficial, these methods usually require substantial investment in the training phase.\n\nAnother test-time scaling method is “self-consistency,” where the model generates multiple responses to the query and chooses the answer that appears more often. Self-consistency reaches its limits when handling complex problems, as in these cases, the most repeated answer is not necessarily the correct one.\n\nSampling-based search offers a simpler and highly scalable alternative to test-time scaling: Let the model generate multiple responses and select the best one through a verification mechanism. Sampling-based search can complement other test-time compute scaling strategies and, as the researchers write in their paper, “it also has the unique advantage of being embarrassingly parallel and allowing for arbitrarily scaling: simply sample more responses.”\n\nMore importantly, sampling-based search can be applied to any LLM, including those that have not been explicitly trained for reasoning.\n\nHow sampling-based search works\n\nThe researchers focus on a minimalist implementation of sampling-based search, using a language model to both generate candidate responses and verify them. This is a “self-verification” process, where the model assesses its own outputs without relying on external ground-truth answers or symbolic verification systems.\n\nSearch-based sampling Credit: VentureBeat\n\nThe algorithm works in a few simple steps:\n\n1—The algorithm begins by generating a set of candidate solutions to the given problem using a language model. This is done by giving the model the same prompt multiple times and using a non-zero temperature setting to create a diverse set of responses.\n\n2—Each candidate’s response undergoes a verification process in which the LLM is prompted multiple times to determine whether the response is correct. The verification outcomes are then averaged to create a final verification score for the response.\n\n3— The algorithm selects the highest-scored response as the final answer. If multiple candidates are within close range of each other, the LLM is prompted to compare them pairwise and choose the best one. The response that wins the most pairwise comparisons is chosen as the final answer.\n\nThe researchers considered two key axes for test-time scaling:\n\nSampling: The number of responses the model generates for each input problem.\n\nVerification: The number of verification scores computed for each generated solution\n\nHow sampling-based search compares to other techniques\n\nThe study revealed that reasoning performance continues to improve with sampling-based search, even when test-time compute is scaled far beyond the point where self-consistency saturates.\n\nAt a sufficient scale, this minimalist implementation significantly boosts reasoning accuracy on reasoning benchmarks like AIME and MATH. For example, Gemini 1.5 Pro’s performance surpassed that of o1-Preview, which has explicitly been trained on reasoning problems, and Gemini 1.5 Flash surpassed Gemini 1.5 Pro.\n\n“This not only highlights the importance of sampling-based search for scaling capability, but also suggests the utility of sampling-based search as a simple baseline on which to compare other test-time compute scaling strategies and measure genuine improvements in models’ search capabilities,” the researchers write.\n\nIt is worth noting that while the results of search-based sampling are impressive, the costs can also become prohibitive. For example, with 200 samples and 50 verification steps per sample, a query from AIME will generate around 130 million tokens, which costs $650 with Gemini 1.5 Pro. However, this is a very minimalistic approach to sampling-based search, and it is compatible with optimization techniques proposed in other studies. With smarter sampling and verification methods, the inference costs can be reduced considerably by using smaller models and generating fewer tokens. For example, by using Gemini 1.5 Flash to perform the verification, the costs drop to $12 per question.\n\nEffective self-verification strategies\n\nThere is an ongoing debate on whether LLMs can verify their own answers. The researchers identified two key strategies for improving self-verification using test-time compute:\n\nDirectly comparing response candidates: Disagreements between candidate solutions strongly indicate potential errors. By providing the verifier with multiple responses to compare, the model can better identify mistakes and hallucinations, addressing a core weakness of LLMs. The researchers describe this as an instance of “implicit scaling.”\n\nTask-specific rewriting: The researchers propose that the optimal output style of an LLM depends on the task. Chain-of-thought is effective for solving reasoning tasks, but responses are easier to verify when written in a more formal, mathematically conventional style. Verifiers can rewrite candidate responses into a more structured format (e.g., theorem-lemma-proof) before evaluation.\n\n“We anticipate model self-verification capabilities to rapidly improve in the short term, as models learn to leverage the principles of implicit scaling and output style suitability, and drive improved scaling rates for sampling-based search,” the researchers write.\n\nImplications for real-world applications\n\nThe study demonstrates that a relatively simple technique can achieve impressive results, potentially reducing the need for complex and costly model architectures or training regimes.\n\nThis is also a scalable technique, enabling enterprises to increase performance by allocating more compute resources to sampling and verification. It also enables developers to push frontier language models beyond their limitations on complex tasks.\n\n“Given that it complements other test-time compute scaling strategies, is parallelizable and allows for arbitrarily scaling, and admits simple implementations that are demonstrably effective, we expect sampling-based search to play a crucial role as language models are tasked with solving increasingly complex problems with increasingly large compute budgets,” the researchers write.",
      "url": "https://venturebeat.com/ai/less-is-more-uc-berkeley-and-google-unlock-llm-potential-through-simple-sampling/",
      "source": "https://venturebeat.com/category/ai/",
      "date": "2025-03-21",
      "authors": [
        "Ben Dickson"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "startup"
    },
    {
      "title": "Artificial Intelligence Webinars",
      "text": "The technical storage or access that is used exclusively for statistical purposes.\n\nThe technical storage or access that is used exclusively for anonymous statistical purposes. Without a subpoena, voluntary compliance on the part of your Internet Service Provider, or additional records from a third party, information stored or retrieved for this purpose alone cannot usually be used to identify you.",
      "url": "https://www.artificialintelligence-news.com/news/videos",
      "source": "https://www.artificialintelligence-news.com/",
      "date": null,
      "authors": [],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "Anthropic's AI assistant Claude learns to search the web",
      "text": "Anthropic has announced its AI assistant Claude can now search the web, providing users with more up-to-date and relevant responses.\n\nThis integration of web search functionality means Claude can now access the latest information to expand its knowledge base beyond its initial training data.\n\nA key feature of this update is the emphasis on transparency and fact-checking. Anthropic highlights that “When Claude incorporates information from the web into its responses, it provides direct citations so you can easily fact check sources.”\n\nFurthermore, Claude aims to streamline the information-gathering process for users. Instead of requiring users to manually sift through search engine results, “Claude processes and delivers relevant sources in a conversational format.”\n\nAnthropic believes this enhancement will unlock a multitude of new use cases for Claude across various industries. They outlined several ways users can leverage Claude with web search:\n\nSales teams: Can now “transform account planning and drive higher win rates through informed conversations with prospects by analysing industry trends to learn key initiatives and pain points.” This allows sales professionals to have more informed and persuasive conversations with potential clients.\n\nCan now “transform account planning and drive higher win rates through informed conversations with prospects by analysing industry trends to learn key initiatives and pain points.” This allows sales professionals to have more informed and persuasive conversations with potential clients. Financial analysts: Can “assess current market data, earnings reports, and industry trends to make better investment decisions and inform financial model assumptions.” Access to real-time financial data can improve the accuracy and timeliness of financial analysis.\n\nCan “assess current market data, earnings reports, and industry trends to make better investment decisions and inform financial model assumptions.” Access to real-time financial data can improve the accuracy and timeliness of financial analysis. Researchers: Can “build stronger grant proposals and literature reviews by searching across primary sources on the web, spotting emerging trends and identifying gaps in the current literature.” This capability can accelerate the research process and lead to more comprehensive and insightful findings.\n\nCan “build stronger grant proposals and literature reviews by searching across primary sources on the web, spotting emerging trends and identifying gaps in the current literature.” This capability can accelerate the research process and lead to more comprehensive and insightful findings. Shoppers: Can “compare product features, prices, and reviews across multiple sources to make more informed purchase decisions.”\n\nWhile the initial rollout is limited to paid users in the US, Anthropic assures that support for users on their free plan and more countries is coming soon.\n\nTo activate the web search feature, users simply need to “toggle on web search in your profile settings and start a conversation with Claude 3.7 Sonnet.” Once enabled, “When applicable, Claude will search the web to inform its response.”\n\nThis update aims to make Claude a more powerful and versatile tool for a wide range of tasks. By providing access to real-time information and ensuring transparency through citations, Anthropic is addressing key challenges and further solidifying Claude’s position as a leading AI assistant.\n\n(Image credit: Anthropic)\n\nSee also: Hugging Face calls for open-source focus in the AI Action Plan\n\nWant to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security & Cloud Expo.\n\nExplore other upcoming enterprise technology events and webinars powered by TechForge here.",
      "url": "https://www.artificialintelligence-news.com/news/anthropic-ai-assistant-claude-learns-search-the-web/",
      "source": "https://www.artificialintelligence-news.com/",
      "date": "2025-03-21",
      "authors": [
        "Ryan Daws"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "Hugging Face calls for open-source focus in the AI Action Plan",
      "text": "Hugging Face has called on the US government to prioritise open-source development in its forthcoming AI Action Plan.\n\nIn a statement to the Office of Science and Technology Policy (OSTP), Hugging Face emphasised that “thoughtful policy can support innovation while ensuring that AI development remains competitive, and aligned with American values.”\n\nHugging Face, which hosts over 1.5 million public models across various sectors and serves seven million users, proposes an AI Action Plan centred on three interconnected pillars:\n\nHugging Face stresses the importance of strengthening open-source AI ecosystems. The company argues that technical innovation stems from diverse actors across institutions and that support for infrastructure – such as the National AI Research Resource (NAIRR), and investment in open science and data – allows these contributions to have an additive effect and accelerate robust innovation.\n\nThe company prioritises efficient and reliable adoption of AI. Hugging Face believes that spreading the benefits of the technology by facilitating its adoption along the value chain requires actors across sectors of activity to shape its development. It states that more efficient, modular, and robust AI models require research and infrastructural investments to enable the broadest possible participation and innovation—enabling diffusion of technology across the US economy.\n\nHugging Face also highlights the need to promote security and standards. The company suggests that decades of practices in open-source software cybersecurity, information security, and standards can inform safer AI technology. It advocates for promoting traceability, disclosure, and interoperability standards to foster a more resilient and robust technology ecosystem.\n\nOpen-source is key for AI advancement in the US (and beyond)\n\nHugging Face underlines that modern AI is built on decades of open research, with commercial giants relying heavily on open-source contributions. Recent breakthroughs – such as OLMO-2 and Olympic-Coder – demonstrate that open research remains a promising path to developing systems that match the performance of commercial models, and can often surpass them, especially in terms of efficiency and performance in specific domains.\n\n“Perhaps most striking is the rapid compression of development timelines,” notes the company, “what once required over 100B parameter models just two years ago can now be accomplished with 2B parameter models, suggesting an accelerating path to parity.”\n\nThis trend towards more accessible, efficient, and collaborative AI development indicates that open approaches to AI development have a critical role to play in enabling a successful AI strategy that maintains technical leadership and supports more widespread and secure adoption of the technology.\n\nHugging Face argues that open models, infrastructure, and scientific practices constitute the foundation of AI innovation, allowing a diverse ecosystem of researchers, companies, and developers to build upon shared knowledge.\n\nThe company’s platform hosts AI models and datasets from both small actors (e.g., startups, universities) and large organisations (e.g., Microsoft, Google, OpenAI, Meta), demonstrating how open approaches accelerate progress and democratise access to AI capabilities.\n\n“The United States must lead in open-source AI and open science, which can enhance American competitiveness by fostering a robust ecosystem of innovation and ensuring a healthy balance of competition and shared innovation,” states Hugging Face.\n\nResearch has shown that open technical systems act as force multipliers for economic impact, with an estimated 2000x multiplier effect. This means that $4 billion invested in open systems could potentially generate $8 trillion in value for companies using them.\n\nThese economic benefits extend to national economies as well. Without any open-source software contributions, the average country would lose 2.2% of its GDP. Open-source drove between €65 billion and €95 billion of European GDP in 2018 alone, a finding so significant that the European Commission cited it when establishing new rules to streamline the process for open-sourcing government software.\n\nThis demonstrates how open-source impact translates directly into policy action and economic advantage at the national level, underlining the importance of open-source as a public good.\n\nPractical factors driving commercial adoption of open-source AI\n\nHugging Face identifies several practical factors driving the commercial adoption of open models:\n\nCost efficiency is a major driver, as developing AI models from scratch requires significant investment, so leveraging open foundations reduces R&D expenses.\n\nis a major driver, as developing AI models from scratch requires significant investment, so leveraging open foundations reduces R&D expenses. Customisation is crucial, as organisations can adapt and deploy models specifically tailored to their use cases rather than relying on one-size-fits-all solutions.\n\nis crucial, as organisations can adapt and deploy models specifically tailored to their use cases rather than relying on one-size-fits-all solutions. Open models reduce vendor lock-in , giving companies greater control over their technology stack and independence from single providers.\n\n, giving companies greater control over their technology stack and independence from single providers. Open models have caught up to and, in certain cases, surpassed the capabilities of closed, proprietary systems.\n\nThese factors are particularly valuable for startups and mid-sized companies, which can access cutting-edge technology without massive infrastructure investments. Banks, pharmaceutical companies, and other industries have been adapting open models to specific market needs—demonstrating how open-source foundations support a vibrant commercial ecosystem across the value chain.\n\nHugging Face’s policy recommendations to support open-source AI in the US\n\nTo support the development and adoption of open AI systems, Hugging Face offers several policy recommendations:\n\nEnhance research infrastructure: Fully implement and expand the National AI Research Resource (NAIRR) pilot. Hugging Face’s active participation in the NAIRR pilot has demonstrated the value of providing researchers with access to computing resources, datasets, and collaborative tools.\n\nAllocate public computing resources for open-source: The public should have ways to participate via public AI infrastructure. One way to do this would be to dedicate a portion of publicly-funded computing infrastructure to support open-source AI projects, reducing barriers to innovation for smaller research teams and companies that cannot afford proprietary systems.\n\nEnable access to data for developing open systems: Create sustainable data ecosystems through targeted policies that address the decreasing data commons. Publishers are increasingly signing data licensing deals with proprietary AI model developers, meaning that quality data acquisition costs are now approaching or even surpassing computational expenses of training frontier models, threatening to lock out small open developers from access to quality data. Support organisations that contribute to public data repositories and streamline compliance pathways that reduce legal barriers to responsible data sharing.\n\nDevelop open datasets: Invest in the creation, curation, and maintenance of robust, representative datasets that can support the next generation of AI research and applications. Expand initiatives like the IBM AI Alliance Trusted Data Catalog and support projects like IDI’s AI-driven Digitization of the public collections in the Boston Public Library.\n\nStrengthen rights-respecting data access frameworks: Establish clear guidelines for data usage, including standardised protocols for anonymisation, consent management, and usage tracking. Support public-private partnerships to create specialised data trusts for high-value domains like healthcare and climate science, ensuring that individuals and organisations maintain appropriate control over their data while enabling innovation.\n\nInvest in stakeholder-driven innovation: Create and support programmes that enable organisations across diverse sectors (healthcare, manufacturing, education) to develop customised AI systems for their specific needs, rather than relying exclusively on general-purpose systems from major providers. This enables broader participation in the AI ecosystem and ensures that the benefits of AI extend throughout the economy.\n\nStrengthen centres of excellence: Expand NIST’s role as a convener for AI experts across academia, industry, and government to share lessons and develop best practices. In particular, the AI Risk Management Framework has played a significant role in identifying stages of AI development and research questions that are critical to ensuring more robust and secure technology deployment for all. The tools developed at Hugging Face, from model documentation to evaluation libraries, are directly shaped by these questions.\n\nSupport high-quality data for performance and reliability evaluation: AI development depends heavily on data, both to train models and to reliably evaluate their progress, strengths, risks, and limitations. Fostering greater access to public data in a safe and secure way and ensuring that the evaluation data used to characterise models is sound and evidence-based will accelerate progress in both performance and reliability of the technology.\n\nPrioritising efficient and reliable AI adoption\n\nHugging Face highlights that smaller companies and startups face significant barriers to AI adoption due to high costs and limited resources. According to IDC, global AI spending will reach $632 billion in 2028, but these costs remain prohibitive for many small organisations.\n\nFor organisations adopting open-source AI tools, it brings financial returns. 51% of surveyed companies currently utilising open-source AI tools report positive ROI, compared to just 41% of those not using open-source.\n\nHowever, energy scarcity presents a growing concern, with the International Energy Agency projecting that data centres’ electricity consumption could double from 2022 levels to 1,000 TWh by 2026, equivalent to Japan’s entire electricity demand. While training AI models is energy-intensive, inference, due to its scale and frequency, can ultimately exceed training energy consumption.\n\nEnsuring broad AI accessibility requires both hardware optimisations and scalable software frameworks. A range of organisations are developing models tailored to their specific needs, and US leadership in efficiency-focused AI development presents a strategic advantage. The DOE’s AI for Energy initiative further supports research into energy-efficient AI, facilitating wider adoption without excessive computational demands.\n\nWith its letter to the OSTP, Hugging Face advocates for an AI Action Plan centred on open-source principles. By taking decisive action, the US can secure its leadership, drive innovation, enhance security, and ensure the widespread benefits of AI are realised across society and the economy.\n\nSee also: UK minister in US to pitch Britain as global AI investment hub\n\nWant to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security & Cloud Expo.\n\nExplore other upcoming enterprise technology events and webinars powered by TechForge here.",
      "url": "https://www.artificialintelligence-news.com/news/hugging-face-open-source-focus-ai-action-plan/",
      "source": "https://www.artificialintelligence-news.com/",
      "date": "2025-03-20",
      "authors": [
        "Ryan Daws"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "UK minister in US to pitch Britain as global AI investment hub",
      "text": "The UK aims to secure its position as a global leader with additional AI investment, with Technology Secretary Peter Kyle currently in the US to champion Britain’s credentials.\n\nAs the UK government prioritises AI within its “Plan for Change,” Kyle’s visit aims to strengthen the special relationship between the UK and the US that has been under particular strain in recent years.\n\nSpeaking at NVIDIA’s annual conference in San Jose on 20th March, Kyle outlined the government’s strategy to “rewire” the British economy around AI. This initiative seeks to distribute the benefits of AI-driven wealth creation beyond traditional hubs like Silicon Valley and London, empowering communities across the UK to embrace its opportunities.\n\nAddressing an audience of business leaders, developers, and innovators, the Technology Secretary articulated his vision for leveraging AI and advanced technologies to tackle complex global challenges, positioning Britain as a beacon of innovation.\n\nThe UK is actively deploying AI to enhance public services and stimulate economic growth, a cornerstone of the government’s “Plan for Change.”\n\nKyle is now highlighting the significant potential of the UK’s AI sector, currently valued at over $92 billion and projected to exceed $1 trillion by 2035. This growth trajectory, according to the government, will position Britain as the second-leading AI nation in the democratic world—presenting a wealth of investment opportunities for US companies and financial institutions.\n\nA central theme of Kyle’s message is the readiness of the UK to embrace AI investment, with a particular emphasis on transforming “the relics of economic eras past into the UK’s innovative AI Growth Zones.”\n\nThese “AI Growth Zones” are a key element of the government’s AI Opportunities Action Plan. They are strategically designated areas designed to rapidly attract large-scale AI investment through streamlined regulations and dedicated infrastructure.\n\nAI Growth Zones, as the name suggests, are envisioned as vibrant hubs for AI development with a pipeline of new opportunities for companies to scale up and innovate. The Technology Secretary is actively encouraging investors to participate in this new form of partnership.\n\nDuring his speech at the NVIDIA conference, Kyle is expected to detail how these Growth Zones – benefiting from access to substantial power connections and a planning system designed to expedite construction – will facilitate the development of a compute infrastructure on a scale that the UK “has never seen before.”\n\nThe government has already received numerous proposals from local leaders and industry stakeholders across the nation, demonstrating Britain’s eagerness to utilise AI to revitalise communities and drive economic growth throughout the country.\n\nThis initiative is expected to contribute to higher living standards across the UK, a key priority for the government over the next four years. The AI Growth Zones are intended to deliver the jobs, investment, and a thriving business environment necessary to improve the financial well-being of citizens and deliver on the “Plan for Change.”\n\nAt the NVIDIA conference, Kyle is expected to say: “In empty factories and abandoned mines, in derelict sites and unused power supplies, I see the places where we can begin to build a new economic model. A model completely rewired around the immense power of artificial intelligence.\n\n“Where, faced with that power, the state is neither a blocker nor a shirker—but an agile, proactive partner. In Britain, we want to turn the relics of economic eras past into AI Growth Zones.”\n\nAs part of his visit to the US, Peter Kyle will also engage with prominent companies in the tech sector, including OpenAI, Anthropic, NVIDIA, and Vantage. His aim is to encourage more of these companies to establish a presence in the UK, positioning it as their “Silicon Valley home from home.”\n\nFurthermore, the Technology Secretary is expected to state: “There is a real hunger for investment in Britain, and people who are optimistic about the future, and hopeful for the opportunities which AI will bring for them and their families. States owe it to their citizens to support it. Not through diktat or directive, but through partnership.”\n\nThe UK Prime Minister and the President of the US have placed AI at the forefront of the transatlantic relationship. During a visit to the White House last month, the Prime Minister confirmed that both nations are collaborating on a new economic deal with advanced technologies at its core.\n\nSince unveiling its new AI strategy at the beginning of the year and assigning the technology a central role in delivering the government’s ‘Plan for Change,’ the UK has already witnessed significant investment from US companies seeking to establish AI bases in Britain.\n\nNotable recent investments include a substantial £12 billion commitment from Vantage Data Centers to significantly expand Britain’s data infrastructure, which is projected to create approximately 11,500 jobs. Additionally, last month saw the UK Government formalise a partnership with Anthropic to enhance collaboration on leveraging AI to improve public services nationwide.\n\nBy strengthening these partnerships with leading US tech firms and investors, the UK’s AI sector is well-positioned for sustained growth as the government aims to continue to remove innovation barriers.\n\n(Photo by Billy Joachim)\n\nSee also: OpenAI and Google call for US government action to secure AI lead\n\nWant to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security & Cloud Expo.\n\nExplore other upcoming enterprise technology events and webinars powered by TechForge here.",
      "url": "https://www.artificialintelligence-news.com/news/uk-minister-in-us-pitch-britain-global-ai-investment-hub/",
      "source": "https://www.artificialintelligence-news.com/",
      "date": "2025-03-20",
      "authors": [
        "Ryan Daws"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "The role of machine learning in enhancing cloud-native container security",
      "text": "The advent of more powerful processors in the early 2000’s started the computing revolution that led to what we now call the cloud. With single hardware instances able to run dozens, if not hundreds of virtual machines concurrently, businesses could offer their users multiple services and applications that would otherwise have been financially impractical, if not impossible.\n\nBut virtual machines (VMs) have several downsides. Often, an entire virtualised operating system is overkill for many applications, and although very much more malleable, scalable, and agile than a fleet of bare-metal servers, VMs still require significantly more memory and processing power, and are less agile than the next evolution of this type of technology – containers. In addition to being more easily scaled (up or down, according to demand), containerised applications consist of only the necessary parts of an application and its supporting dependencies. Therefore apps based on micro-services tend to be lighter and more easily configurable.\n\nVirtual machines exhibit the same security issues that affect their bare-metal counterparts, and to some extent, container security issues reflect those of their component parts: a mySQL bug in a specific version of the upstream application will affect containerised versions too. With regards to VMs, bare metal installs, and containers, cybersecurity concerns and activities are very similar. But container deployments and their tooling bring specific security challenges to those charged with running apps and services, whether manually piecing together applications with choice containers, or running in production with orchestration at scale.\n\nContainer-specific security risks\n\nMisconfiguration: Complex applications are made up of multiple containers, and misconfiguration – often only a single line in a .yaml file, can grant unnecessary privileges and increase the attack surface. For example, although it’s not trivial for an attacker to gain root access to the host machine from a container, it’s still a too-common practice to run Docker as root, with no user namespace remapping, for example.\n\nVulnerable container images: In 2022, Sysdig found over 1,600 images identified as malicious in Docker Hub, in addition to many containers stored in the repo with hard-coded cloud credentials, ssh keys, and NPM tokens. The process of pulling images from public registries is opaque, and the convenience of container deployment (plus pressure on developers to produce results, fast) can mean that apps can easily be constructed with inherently insecure, or even malicious components.\n\nOrchestration layers: For larger projects, orchestration tools such as Kubernetes can increase the attack surface, usually due to misconfiguration and high levels of complexity. A 2022 survey from D2iQ found that only 42% of applications running on Kubernetes made it into production – down in part to the difficulty of administering large clusters and a steep learning curve.\n\nAccording to Ari Weil at Akamai, “Kubernetes is mature, but most companies and developers don’t realise how complex […] it can be until they’re actually at scale.”\n\nContainer security with machine learning\n\nThe specific challenges of container security can be addressed using machine learning algorithms trained on observing the components of an application when it’s ‘running clean.’ By creating a baseline of normal behaviour, machine learning can identify anomalies that could indicate potential threats from unusual traffic, unauthorised changes to configuration, odd user access patterns, and unexpected system calls.\n\nML-based container security platforms can scan image repositories and compare each against databases of known vulnerabilities and issues. Scans can be automatically triggered and scheduled, helping prevent the addition of harmful elements during development and in production. Auto-generated audit reports can be tracked against standard benchmarks, or an organisation can set its own security standards – useful in environments where highly-sensitive data is processed.\n\nThe connectivity between specialist container security functions and orchestration software means that suspected containers can be isolated or closed immediately, insecure permissions revoked, and user access suspended. With API connections to local firewalls and VPN endpoints, entire environments or subnets can be isolated, or traffic stopped at network borders.\n\nFinal word\n\nMachine learning can reduce the risk of data breach in containerised environments by working on several levels. Anomaly detection, asset scanning, and flagging potential misconfiguration are all possible, plus any degree of automated alerting or amelioration are relatively simple to enact.\n\nThe transformative possibilities of container-based apps can be approached without the security issues that have stopped some from exploring, developing, and running microservice-based applications. The advantages of cloud-native technologies can be won without compromising existing security standards, even in high-risk sectors.\n\n(Image source)",
      "url": "https://www.artificialintelligence-news.com/news/the-role-of-machine-learning-in-enhancing-cloud-native-container-security/",
      "source": "https://www.artificialintelligence-news.com/",
      "date": "2025-02-12",
      "authors": [
        "Ai News"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "Welcome to the new and improved Verge Deals newsletter",
      "text": "Hey, folks! Every week for the past four years, the team behind Verge Deals has combed the web, looking for the best deals and discounts on the tech we love most at The Verge. We pride ourselves in having tried and tested every product we recommend — well, almost every product — and we continue to share those deals with our readers via our daily deal coverage and Verge Deals newsletter. That being said, everyone could use a little change every now and again.\n\nNo, Verge Deals is not going away — quite the contrary, actually. We’ve given our newsletter a fresh coat of virtual paint to reflect our new(ish) colors and design language, and we plan to continue to deliver a fresh batch of deals to your inbox every Friday afternoon. This time, though, we’re incorporating guest dispatches from the larger Verge staff, more personal gift recommendations, and more subscriber exclusives.\n\nWe’ll still highlight steep price drops on Verge-approved favorites like the latest 13-inch MacBook Pro and Sony’s WH-1000MX5 headphones, as well as other recs culled from our expert reviews and buying guides. We’ll even tell you how you can land limited-edition tech that’s likely to sell out — including Sony’s forthcoming PlayStation 5 Pro.\n\nIf you don’t already subscribe to Verge Deals, you can sign up for free using the box below. We’re going to keep the weekly cadence (for now), but you can expect bimonthly special sends in the run-up to Amazon’s next Prime Day event, Black Friday, and the holiday shopping season.\n\nKeep an eye on your inbox this afternoon for the first issue of the new and improved Verge Deals, and if you’re curious about The Verge’s full slate of both free and premium newsletters, head over to our newsletter hub to sign up for Command Line, Installer, and Notepad. We’re excited about the changes in the coming months. We hope you are, too.\n\nVerge Deals Sign up for Verge Deals to get deals on products we’ve tested sent to your inbox weekly. Email (required) Sign Up By submitting your email, you agree to our Terms and Privacy Notice . This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.",
      "url": "https://www.theverge.com/2024/9/20/24249294/verge-deals-newsletter-subscribe-tech-discounts",
      "source": "https://www.theverge.com/ai-artificial-intelligence",
      "date": "2024-09-20",
      "authors": [
        "Brandon Widder"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "Trump’s confusing crusade against Big Tech",
      "text": "After all that, it’s time for some policy talk. The Verge’s Lauren Feiner joins the show to explain the latest on the TikTok ban, the state of the Broadband Equity, Access, and Deployment program, what is happening at the FTC, why Brendan Carr continues to be the way he is, the fight over AI copyright, and much more.",
      "url": "https://www.theverge.com/the-vergecast/633757/trump-big-tech-tiktok-ftc-siri-vergecast",
      "source": "https://www.theverge.com/ai-artificial-intelligence",
      "date": "2025-03-21",
      "authors": [
        "David Pierce"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "news"
    },
    {
      "title": "Trump’s confusing crusade against Big Tech",
      "text": "After all that, it’s time for some policy talk. The Verge’s Lauren Feiner joins the show to explain the latest on the TikTok ban, the state of the Broadband Equity, Access, and Deployment program, what is happening at the FTC, why Brendan Carr continues to be the way he is, the fight over AI copyright, and much more.",
      "url": "https://www.theverge.com/the-vergecast/633757/trump-big-tech-tiktok-ftc-siri-vergecast#comments",
      "source": "https://www.theverge.com/ai-artificial-intelligence",
      "date": "2025-03-21",
      "authors": [
        "David Pierce"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "news"
    },
    {
      "title": "Switch 2, Steam Deck, and the next-gen console wars",
      "text": "The Nintendo Switch 2 is nigh. Nigh-ish, anyway. When it does launch later this year, it might start another revolution in portable gaming, as millions of people are able to play from their couch, their bed, the train, and anywhere else. That was also true of the first Switch, though, and as big a hit as it was it didn’t spark an industry-wide shift to handhelds the way some people expected. So what gives?\n\nAfter that, David reports on a trip he took to Florida to see behind the scenes at TGL, a golf league featuring some of the highest-tech sporting gear you’ve ever seen anywhere. Tiger Woods, Rory McIlroy, and some of the other biggest names in golf are betting big that this combination of video games, augmented reality, and live action can be a hit both on TV and in your social feeds. But it also raises some questions, like: how do you know when you can trust the computer? And how big is a 53-foot-tall screen, really?\n\nFinally, we answer a question from the Vergecast Hotline (call 866-VERGE11 or email vergecast@theverge.com!) about why you might upgrade from a normal iPad to an iPad Air. We’ve heard from a lot of students and note-takers the last few weeks who say the extra stylus support, and a few power-hungry apps, make the extra expense worth it.\n\nIf you want to know more about everything we discuss in this episode, here are some links to get you started:",
      "url": "https://www.theverge.com/the-vergecast/631619/switch-2-steam-deck-handheld-consoles-vergecast",
      "source": "https://www.theverge.com/ai-artificial-intelligence",
      "date": "2025-03-18",
      "authors": [
        "David Pierce"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "startup"
    },
    {
      "title": "Switch 2, Steam Deck, and the next-gen console wars",
      "text": "The Nintendo Switch 2 is nigh. Nigh-ish, anyway. When it does launch later this year, it might start another revolution in portable gaming, as millions of people are able to play from their couch, their bed, the train, and anywhere else. That was also true of the first Switch, though, and as big a hit as it was it didn’t spark an industry-wide shift to handhelds the way some people expected. So what gives?\n\nAfter that, David reports on a trip he took to Florida to see behind the scenes at TGL, a golf league featuring some of the highest-tech sporting gear you’ve ever seen anywhere. Tiger Woods, Rory McIlroy, and some of the other biggest names in golf are betting big that this combination of video games, augmented reality, and live action can be a hit both on TV and in your social feeds. But it also raises some questions, like: how do you know when you can trust the computer? And how big is a 53-foot-tall screen, really?\n\nFinally, we answer a question from the Vergecast Hotline (call 866-VERGE11 or email vergecast@theverge.com!) about why you might upgrade from a normal iPad to an iPad Air. We’ve heard from a lot of students and note-takers the last few weeks who say the extra stylus support, and a few power-hungry apps, make the extra expense worth it.\n\nIf you want to know more about everything we discuss in this episode, here are some links to get you started:",
      "url": "https://www.theverge.com/the-vergecast/631619/switch-2-steam-deck-handheld-consoles-vergecast#comments",
      "source": "https://www.theverge.com/ai-artificial-intelligence",
      "date": "2025-03-18",
      "authors": [
        "David Pierce"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "startup"
    },
    {
      "title": "Nvidia GTC 2025 live updates: Blackwell Ultra, GM partnerships, and two ‘personal AI supercomputers’",
      "text": "Nvidia announced Thursday it’s partnering with EPRI, a power industry R&D organization, to use AI to solve problems facing the electrical grid. Perhaps ironically, the issues are largely caused by rising power demand from AI itself.\n\nThe Open Power AI Consortium, which includes a number of electrical utilities and tech companies, says it will use domain-specific AI models to devise new ways to tackle problems that the power industry is predicted to face in the coming years. The models will be open sourced and available to researchers across academia and industry.\n\nThe power industry is facing surging demand from data centers in the United States and elsewhere as AI ramps up the need for computing power. Electricity demand is expected to grow by 4% annually in the coming years, according to the International Energy Agency, nearly double over 2023 figures.",
      "url": "https://techcrunch.com/storyline/nvidia-gtc-2025-live-updates-blackwell-ultra-next-gen-rubin-chip-architecture-and-much-more/",
      "source": "https://techcrunch.com/category/artificial-intelligence/",
      "date": null,
      "authors": [
        "Tim De Chant",
        "Charles Rollet",
        "Kirsten Korosec",
        "Kyle Wiggers",
        "Maxwell Zeff",
        "Natasha Lomas",
        "Sean O'Kane",
        "Amanda Silberling",
        "Cody Corrall",
        "Alyssa Stringer"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "research"
    },
    {
      "title": "Meta has revenue sharing agreements with Llama AI model hosts, filing reveals",
      "text": "In a blog post last July, Meta CEO Mark Zuckerberg said that “selling access” to Meta’s openly available Llama AI models “isn’t [Meta’s] business model.” Yet Meta does make at least some money from Llama through revenue-sharing agreements, according to a newly unredacted court filing.\n\nThe filing, submitted by attorneys for the plaintiffs in the copyright lawsuit Kadrey v. Meta, in which Meta stands accused of training its Llama models on hundreds of terabytes of pirated e-books, reveals that Meta “shares a percentage of the revenue” that companies hosting its Llama models generate from users of those models.\n\nThe filing doesn’t indicate which specific hosts pay Meta. But Meta lists a number of Llama host partners in various blog posts, including AWS, Nvidia, Databricks, Groq, Dell, Azure, Google Cloud, and Snowflake.\n\nDevelopers aren’t required to use a Llama model through a host partner. The models can be downloaded, fine-tuned, and run on a range of different hardware. But many hosts provide additional services and tooling that makes getting Llama models up and running simpler and easier.\n\nZuckerberg mentioned the possibility of licensing access to Llama models during an earnings call last April, when he also floated monetizing Llama in other ways, like through business messaging services and ads in “AI interactions.” But he didn’t outline specifics.\n\n“[I]f you’re someone like Microsoft or Amazon or Google and you’re going to basically be reselling these services, that’s something that we think we should get some portion of the revenue for,” Zuckerberg said. “So those are the deals that we intend to be making, and we’ve started doing that a little bit.”\n\nMore recently, Zuckerberg asserted that most of the value Meta derives from Llama comes in the form of improvements to the models from the AI research community. Meta uses Llama models to power a number of products across its platforms and properties, including Meta’s AI assistant, Meta AI.\n\n“I think it’s good business for us to do this in an open way,” Zuckerberg said during Meta’s Q3 2024 earnings call. “[I]t makes our products better rather than if we were just on an island building a model that no one was kind of standardizing around in the industry.”\n\nThe fact that Meta may generate revenue in a rather direct way from Llama is significant because plaintiffs in Kadrey v. Meta claim that Meta not only used pirated works to develop Llama, but facilitated infringement by “seeding,” or uploading, these works. Plaintiffs allege that Meta used surreptitious torrenting methods to obtain e-books for training, and in the process — due to the way torrenting works — shared the e-books with other torrenters.\n\nMeta plans to significantly up its capital expenditures this year, largely thanks to its increasing investments in AI. In January, the company said it would spend $60 billion-$80 billion on CapEx in 2025 — roughly double Meta’s CapEx in 2024 — primarily on data centers and growing the company’s AI development teams.\n\nLikely to offset a portion of the costs, Meta is reportedly considering launching a subscription service for Meta AI that’ll add unspecified capabilities to the assistant.\n\nUpdated 3/21 at 1:54 p.m.: A Meta spokesperson pointed TechCrunch to this earnings call transcript for additional context. We’ve added a Zuckerberg quote from it — specifically a quote about Meta’s intent to revenue share with large hosts of Llama models.",
      "url": "https://techcrunch.com/2025/03/21/meta-has-revenue-sharing-agreements-with-llama-ai-model-hosts-filing-reveals/",
      "source": "https://techcrunch.com/category/artificial-intelligence/",
      "date": "2025-03-21",
      "authors": [
        "Kyle Wiggers",
        "Ai Editor",
        "Natasha Lomas",
        "Charles Rollet",
        "Sean O'Kane",
        "Amanda Silberling",
        "Cody Corrall",
        "Alyssa Stringer",
        "Kate Park",
        "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "startup"
    },
    {
      "title": "DeepSeek: Everything you need to know about the AI chatbot app",
      "text": "DeepSeek has gone viral.\n\nChinese AI lab DeepSeek broke into the mainstream consciousness this week after its chatbot app rose to the top of the Apple App Store charts (and Google Play, as well). DeepSeek’s AI models, which were trained using compute-efficient techniques, have led Wall Street analysts — and technologists — to question whether the U.S. can maintain its lead in the AI race and whether the demand for AI chips will sustain.\n\nBut where did DeepSeek come from, and how did it rise to international fame so quickly?\n\nDeepSeek’s trader origins\n\nDeepSeek is backed by High-Flyer Capital Management, a Chinese quantitative hedge fund that uses AI to inform its trading decisions.\n\nAI enthusiast Liang Wenfeng co-founded High-Flyer in 2015. Wenfeng, who reportedly began dabbling in trading while a student at Zhejiang University, launched High-Flyer Capital Management as a hedge fund in 2019 focused on developing and deploying AI algorithms.\n\nIn 2023, High-Flyer started DeepSeek as a lab dedicated to researching AI tools separate from its financial business. With High-Flyer as one of its investors, the lab spun off into its own company, also called DeepSeek.\n\nFrom day one, DeepSeek built its own data center clusters for model training. But like other AI companies in China, DeepSeek has been affected by U.S. export bans on hardware. To train one of its more recent models, the company was forced to use Nvidia H800 chips, a less-powerful version of a chip, the H100, available to U.S. companies.\n\nDeepSeek’s technical team is said to skew young. The company reportedly aggressively recruits doctorate AI researchers from top Chinese universities. DeepSeek also hires people without any computer science background to help its tech better understand a wide range of subjects, per The New York Times.\n\nDeepSeek’s strong models\n\nDeepSeek unveiled its first set of models — DeepSeek Coder, DeepSeek LLM, and DeepSeek Chat — in November 2023. But it wasn’t until last spring, when the startup released its next-gen DeepSeek-V2 family of models, that the AI industry started to take notice.\n\nDeepSeek-V2, a general-purpose text- and image-analyzing system, performed well in various AI benchmarks — and was far cheaper to run than comparable models at the time. It forced DeepSeek’s domestic competition, including ByteDance and Alibaba, to cut the usage prices for some of their models, and make others completely free.\n\nDeepSeek-V3, launched in December 2024, only added to DeepSeek’s notoriety.\n\nAccording to DeepSeek’s internal benchmark testing, DeepSeek V3 outperforms both downloadable, openly available models like Meta’s Llama and “closed” models that can only be accessed through an API, like OpenAI’s GPT-4o.\n\nEqually impressive is DeepSeek’s R1 “reasoning” model. Released in January, DeepSeek claims R1 performs as well as OpenAI’s o1 model on key benchmarks.\n\nBeing a reasoning model, R1 effectively fact-checks itself, which helps it to avoid some of the pitfalls that normally trip up models. Reasoning models take a little longer — usually seconds to minutes longer — to arrive at solutions compared to a typical non-reasoning model. The upside is that they tend to be more reliable in domains such as physics, science, and math.\n\nThere is a downside to R1, DeepSeek V3, and DeepSeek’s other models, however. Being Chinese-developed AI, they’re subject to benchmarking by China’s internet regulator to ensure that its responses “embody core socialist values.” In DeepSeek’s chatbot app, for example, R1 won’t answer questions about Tiananmen Square or Taiwan’s autonomy.\n\nA disruptive approach\n\nIf DeepSeek has a business model, it’s not clear what that model is, exactly. The company prices its products and services well below market value — and gives others away for free. It’s also not taking investor money, despite a ton of VC interest.\n\nThe way DeepSeek tells it, efficiency breakthroughs have enabled it to maintain extreme cost competitiveness. Some experts dispute the figures the company has supplied, however.\n\nWhatever the case may be, developers have taken to DeepSeek’s models, which aren’t open source as the phrase is commonly understood but are available under permissive licenses that allow for commercial use. According to Clem Delangue, the CEO of Hugging Face, one of the platforms hosting DeepSeek’s models, developers on Hugging Face have created over 500 “derivative” models of R1 that have racked up 2.5 million downloads combined.\n\nDeepSeek’s success against larger and more established rivals has been described as “upending AI” and “over-hyped.” The company’s success was at least in part responsible for causing Nvidia’s stock price to drop by 18% in January, and for eliciting a public response from OpenAI CEO Sam Altman. In March, U.S. Commerce department bureaus told staffers that DeepSeek will be banned on their government devices, according to Reuters.\n\nMicrosoft announced that DeepSeek is available on its Azure AI Foundry service, Microsoft’s platform that brings together AI services for enterprises under a single banner. When asked about DeepSeek’s impact on Meta’s AI spending during its first-quarter earnings call, CEO Mark Zuckerberg said spending on AI infrastructure will continue to be a “strategic advantage” for Meta. In March, OpenAI called DeepSeek “state-subsidized” and “state-controlled,” and recommends that the U.S. government consider banning models from DeepSeek.\n\nDuring Nvidia’s fourth-quarter earnings call, CEO Jensen Huang emphasized DeepSeek’s “excellent innovation,” saying that it and other “reasoning” models are great for Nvidia because they need so much more compute.\n\nAt the same time, some companies are banning DeepSeek, and so are entire countries and governments, including South Korea. New York state also banned DeepSeek from being used on government devices.\n\nAs for what DeepSeek’s future might hold, it’s not clear. Improved models are a given. But the U.S. government appears to be growing wary of what it perceives as harmful foreign influence. In March, The Wall Street Journal reported that the U.S. will likely ban DeepSeek on government devices.\n\nThis story was originally published January 28, 2025, and will be updated regularly.",
      "url": "https://techcrunch.com/2025/03/21/deepseek-everything-you-need-to-know-about-the-ai-chatbot-app/",
      "source": "https://techcrunch.com/category/artificial-intelligence/",
      "date": "2025-03-21",
      "authors": [
        "Kyle Wiggers",
        "Ai Editor",
        "Natasha Lomas",
        "Charles Rollet",
        "Sean O'Kane",
        "Amanda Silberling",
        "Cody Corrall",
        "Alyssa Stringer",
        "Kate Park",
        "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "startup"
    },
    {
      "title": "Anthropic appears to be using Brave to power web search for its Claude chatbot",
      "text": "\n\nEarlier this week, Anthropic rolled out a web search feature for its AI-powered chatbot platform, Claude, bringing the bot in line with many of its rivals. It wasn’t immediately clear which search index might be powering the feature — one possibility was that Anthropic had developed its own. But evidence suggests it’s Brave Search, the search engine maintained by browser developer Brave.\n\nAs spotted by software engineer Antonio Zugaldia on Friday, Anthropic added “Brave Search” to the “subprocessor list” in its documentation this week — the list of Anthropic partners who process Claude data. British programmer Simon Willison reports that at least one search in Claude and Brave returned identical citations. Willison also found that Claude’s web search function contains a parameter called “BraveSearchParams.”\n\nWe’ve reached out to Anthropic and will update this post if we hear back.\n\nBrave underpins at least one other chatbot’s search functionality: Mistral’s chatbot platform Le Chat. In February, Brave and Mistral announced that Le Chat would use Brave’s search API for live web results.\n\nSome AI companies keep info about their search index partnerships close to the chest, possibly for competitive reasons. OpenAI has a partnership with Bing but uses other undisclosed sources to power the search experience in ChatGPT as well.",
      "url": "https://techcrunch.com/2025/03/21/anthropic-appears-to-be-using-brave-to-power-web-searches-for-its-claude-chatbot/",
      "source": "https://techcrunch.com/category/artificial-intelligence/",
      "date": "2025-03-21",
      "authors": [
        "Kyle Wiggers",
        "Ai Editor",
        "Natasha Lomas",
        "Charles Rollet",
        "Sean O'Kane",
        "Amanda Silberling",
        "Cody Corrall",
        "Alyssa Stringer",
        "Kate Park",
        "--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "news"
    },
    {
      "title": "ChatGPT: Everything you need to know about the AI chatbot",
      "text": "ChatGPT, OpenAI’s text-generating AI chatbot, has taken the world by storm since its launch in November 2022. What started as a tool to supercharge productivity through writing essays and code with short text prompts has evolved into a behemoth with 300 million weekly active users.\n\n2024 was a big year for OpenAI, from its partnership with Apple for its generative AI offering, Apple Intelligence, the release of GPT-4o with voice capabilities, and the highly-anticipated launch of its text-to-video model Sora.\n\nOpenAI also faced its share of internal drama, including the notable exits of high-level execs like co-founder and longtime chief scientist Ilya Sutskever and CTO Mira Murati. OpenAI has also been hit with lawsuits from Alden Global Capital-owned newspapers alleging copyright infringement, as well as an injunction from Elon Musk to halt OpenAI’s transition to a for-profit.\n\nIn 2025, OpenAI is battling the perception that it’s ceding ground in the AI race to Chinese rivals like DeepSeek. The company has been trying to shore up its relationship with Washington as it simultaneously pursues an ambitious data center project, and as it reportedly lays the groundwork for one of the largest funding rounds in history.\n\nBelow, you’ll find a timeline of ChatGPT product updates and releases, starting with the latest, which we’ve been updating throughout the year. If you have any other questions, check out our ChatGPT FAQ here.\n\nMarch 2025\n\nOpenAI faces privacy complaint in Europe for chatbot’s defamatory hallucinations\n\nNoyb, a privacy rights advocacy group, is supporting an individual in Norway who was shocked to discover that ChatGPT was providing false information about him, stating that he had been found guilty of killing two of his children and trying to harm the third. “The GDPR is clear. Personal data has to be accurate,” said Joakim Söderberg, data protection lawyer at Noyb, in a statement. “If it’s not, users have the right to have it changed to reflect the truth. Showing ChatGPT users a tiny disclaimer that the chatbot can make mistakes clearly isn’t enough. You can’t just spread false information and in the end add a small disclaimer saying that everything you said may just not be true.”\n\nOpenAI upgrades its transcription and voice-generating AI models\n\nOpenAI has added new transcription and voice-generating AI models to its APIs: a text-to-speech model, “gpt-4o-mini-tts,” that delivers more nuanced and realistic sounding speech, as well as two speech-to-text models called “gpt-4o-transcribe” and “gpt-4o-mini-transcribe”. The company claims they are improved versions of what was already there and that they hallucinate less.\n\nOpenAI has launched o1-pro, a more powerful version of its o1\n\nOpenAI has introduced o1-pro in its developer API. OpenAI says its o1-pro uses more computing than its o1 “reasoning” AI model to deliver “consistently better responses.” It’s only accessible to select developers who have spent at least $5 on OpenAI API services. OpenAI charges $150 for every million tokens (about 750,000 words) input into the model and $600 for every million tokens the model produces. It costs twice as much as OpenAI’s GPT-4.5 for input and 10 times the price of regular o1.\n\nOpenAI research lead Noam Brown thinks AI “reasoning” models could’ve arrived decades ago\n\nNoam Brown, who heads AI reasoning research at OpenAI, thinks that certain types of AI models for “reasoning” could have been developed 20 years ago if researchers had understood the correct approach and algorithms.\n\nOpenAI says it has trained an AI that’s “really good” at creative writing\n\nOpenAI CEO Sam Altman said, in a post on X, that the company has trained a “new model” that’s “really good” at creative writing. He posted a lengthy sample from the model given the prompt “Please write a metafictional literary short story about AI and grief.” OpenAI has not extensively explored the use of AI for writing fiction. The company has mostly concentrated on challenges in rigid, predictable areas such as math and programming. And it turns out that it might not be that great at creative writing at all.\n\nwe trained a new model that is good at creative writing (not sure yet how/when it will get released). this is the first time i have been really struck by something written by AI; it got the vibe of metafiction so right.\n\n\n\nPROMPT:\n\n\n\nPlease write a metafictional literary short story… — Sam Altman (@sama) March 11, 2025\n\nOpenAI rolled out new tools designed to help developers and businesses build AI agents — automated systems that can independently accomplish tasks — using the company’s own AI models and frameworks. The tools are part of OpenAI’s new Responses API, which enables enterprises to develop customized AI agents that can perform web searches, scan through company files, and navigate websites, similar to OpenAI’s Operator product. The Responses API effectively replaces OpenAI’s Assistants API, which the company plans to discontinue in the first half of 2026.\n\nOpenAI reportedly plans to charge up to $20,000 a month for specialized AI ‘agents’\n\nOpenAI intends to release several “agent” products tailored for different applications, including sorting and ranking sales leads and software engineering, according to a report from The Information. One, a “high-income knowledge worker” agent, will reportedly be priced at $2,000 a month. Another, a software developer agent, is said to cost $10,000 a month. The most expensive rumored agents, which are said to be aimed at supporting “PhD-level research,” are expected to cost $20,000 per month. The jaw-dropping figure is indicative of how much cash OpenAI needs right now: The company lost roughly $5 billion last year after paying for costs related to running its services and other expenses. It’s unclear when these agentic tools might launch or which customers will be eligible to buy them.\n\nChatGPT can directly edit your code\n\nThe latest version of the macOS ChatGPT app allows users to edit code directly in supported developer tools, including Xcode, VS Code, and JetBrains. ChatGPT Plus, Pro, and Team subscribers can use the feature now, and the company plans to roll it out to more users like Enterprise, Edu, and free users.\n\nChatGPT’s weekly active users doubled in less than 6 months, thanks to new releases\n\nAccording to a new report from VC firm Andreessen Horowitz (a16z), OpenAI’s AI chatbot, ChatGPT, experienced solid growth in the second half of 2024. It took ChatGPT nine months to increase its weekly active users from 100 million in November 2023 to 200 million in August 2024, but it only took less than six months to double that number once more, according to the report. ChatGPT’s weekly active users increased to 300 million by December 2024 and 400 million by February 2025. ChatGPT has experienced significant growth recently due to the launch of new models and features, such as GPT-4o, with multimodal capabilities. ChatGPT usage spiked from April to May 2024, shortly after that model’s launch.\n\nFebruary 2025\n\nOpenAI cancels its o3 AI model in favor of a ‘unified’ next-gen release\n\nOpenAI has effectively canceled the release of o3 in favor of what CEO Sam Altman is calling a “simplified” product offering. In a post on X, Altman said that, in the coming months, OpenAI will release a model called GPT-5 that “integrates a lot of [OpenAI’s] technology,” including o3, in ChatGPT and its API. As a result of that roadmap decision, OpenAI no longer plans to release o3 as a standalone model.\n\nChatGPT may not be as power-hungry as once assumed\n\nA commonly cited stat is that ChatGPT requires around 3 watt-hours of power to answer a single question. Using OpenAI’s latest default model for ChatGPT, GPT-4o, as a reference, nonprofit AI research institute Epoch AI found the average ChatGPT query consumes around 0.3 watt-hours. However, the analysis doesn’t consider the additional energy costs incurred by ChatGPT with features like image generation or input processing.\n\nOpenAI now reveals more of its o3-mini model’s thought process\n\nIn response to pressure from rivals like DeepSeek, OpenAI is changing the way its o3-mini model communicates its step-by-step “thought” process. ChatGPT users will see an updated “chain of thought” that shows more of the model’s “reasoning” steps and how it arrived at answers to questions.\n\nYou can now use ChatGPT web search without logging in\n\nOpenAI is now allowing anyone to use ChatGPT web search without having to log in. While OpenAI had previously allowed users to ask ChatGPT questions without signing in, responses were restricted to the chatbot’s last training update. This only applies through ChatGPT.com, however. To use ChatGPT in any form through the native mobile app, you will still need to be logged in.\n\nOpenAI unveils a new ChatGPT agent for ‘deep research’\n\nOpenAI announced a new AI “agent” called deep research that’s designed to help people conduct in-depth, complex research using ChatGPT. OpenAI says the “agent” is intended for instances where you don’t just want a quick answer or summary, but instead need to assiduously consider information from multiple websites and other sources.\n\nFebruary 2025\n\nOpenAI used a subreddit to test AI persuasion\n\nOpenAI used the subreddit r/ChangeMyView to measure the persuasive abilities of its AI reasoning models. OpenAI says it collects user posts from the subreddit and asks its AI models to write replies, in a closed environment, that would change the Reddit user’s mind on a subject. The company then shows the responses to testers, who assess how persuasive the argument is, and finally OpenAI compares the AI models’ responses to human replies for that same post.\n\nOpenAI launches o3-mini, its latest ‘reasoning’ model\n\nOpenAI launched a new AI “reasoning” model, o3-mini, the newest in the company’s o family of models. OpenAI first previewed the model in December alongside a more capable system called o3. OpenAI is pitching its new model as both “powerful” and “affordable.”\n\nChatGPT’s mobile users are 85% male, report says\n\nA new report from app analytics firm Appfigures found that over half of ChatGPT’s mobile users are under age 25, with users between ages 50 and 64 making up the second largest age demographic. The gender gap among ChatGPT users is even more significant. Appfigures estimates that across age groups, men make up 84.5% of all users.\n\nOpenAI launches ChatGPT plan for US government agencies\n\nOpenAI launched ChatGPT Gov designed to provide U.S. government agencies an additional way to access the tech. ChatGPT Gov includes many of the capabilities found in OpenAI’s corporate-focused tier, ChatGPT Enterprise. OpenAI says that ChatGPT Gov enables agencies to more easily manage their own security, privacy, and compliance, and could expedite internal authorization of OpenAI’s tools for the handling of non-public sensitive data.\n\nMore teens report using ChatGPT for schoolwork, despite the tech’s faults\n\nYounger Gen Zers are embracing ChatGPT, for schoolwork, according to a new survey by the Pew Research Center. In a follow-up to its 2023 poll on ChatGPT usage among young people, Pew asked ~1,400 U.S.-based teens ages 13 to 17 whether they’ve used ChatGPT for homework or other school-related assignments. Twenty-six percent said that they had, double the number two years ago. Just over half of teens responding to the poll said they think it’s acceptable to use ChatGPT for researching new subjects. But considering the ways ChatGPT can fall short, the results are possibly cause for alarm.\n\nOpenAI says it may store deleted Operator data for up to 90 days\n\nOpenAI says that it might store chats and associated screenshots from customers who use Operator, the company’s AI “agent” tool, for up to 90 days — even after a user manually deletes them. While OpenAI has a similar deleted data retention policy for ChatGPT, the retention period for ChatGPT is only 30 days, which is 60 days shorter than Operator’s.\n\nOpenAI launches Operator, an AI agent that performs tasks autonomously\n\nOpenAI is launching a research preview of Operator, a general-purpose AI agent that can take control of a web browser and independently perform certain actions. Operator promises to automate tasks such as booking travel accommodations, making restaurant reservations, and shopping online.\n\nOpenAI may preview its agent tool for users on the $200-per-month Pro plan\n\nOperator, OpenAI’s agent tool, could be released sooner rather than later. Changes to ChatGPT’s code base suggest that Operator will be available as an early research preview to users on the $200 Pro subscription plan. The changes aren’t yet publicly visible, but a user on X who goes by Choi spotted these updates in ChatGPT’s client-side code. TechCrunch separately identified the same references to Operator on OpenAI’s website.\n\nOpenAI tests phone number-only ChatGPT signups\n\nOpenAI has begun testing a feature that lets new ChatGPT users sign up with only a phone number — no email required. The feature is currently in beta in the U.S. and India. However, users who create an account using their number can’t upgrade to one of OpenAI’s paid plans without verifying their account via an email. Multi-factor authentication also isn’t supported without a valid email.\n\nChatGPT now lets you schedule reminders and recurring tasks\n\nChatGPT’s new beta feature, called tasks, allows users to set simple reminders. For example, you can ask ChatGPT to remind you when your passport expires in six months, and the AI assistant will follow up with a push notification on whatever platform you have tasks enabled. The feature will start rolling out to ChatGPT Plus, Team, and Pro users around the globe this week.\n\nNew ChatGPT feature lets users assign it traits like ‘chatty’ and ‘Gen Z’\n\nOpenAI is introducing a new way for users to customize their interactions with ChatGPT. Some users found they can specify a preferred name or nickname and “traits” they’d like the chatbot to have. OpenAI suggests traits like “Chatty,” “Encouraging,” and “Gen Z.” However, some users reported that the new options have disappeared, so it’s possible they went live prematurely.\n\nDecember 2024\n\nChatGPT Search can be tricked into misleading users, new research reveals\n\nChatGPT Search can be fooled into generating completely misleading summaries, The Guardian has found. They found ChatGPT could be prompted to ignore negative reviews and generate “entirely positive” summaries by inserting hidden text into websites it created and that ChatGPT Search could also be made to spit out malicious code using this method.\n\nMicrosoft and OpenAI reportedly have a finance-centric definition of AGI\n\nMicrosoft and OpenAI have a very specific, internal definition of AGI based on the startup’s profits, according to a new report from The Information. The two companies reportedly signed an agreement stating OpenAI has only achieved AGI when it develops AI systems that can generate at least $100 billion in profit, which is far from the rigorous technical and philosophical definition of AGI many would expect.\n\nOpenAI trained o1 and o3 to ‘think’ about its safety policy\n\nOpenAI released new research outlining the company’s approach to ensure AI reasoning models stay aligned with the values of their human developers. The startup used “deliberative alignment” to make o1 and o3 “think” about OpenAI’s safety policy. According to OpenAI’s research, the method decreased the rate at which o1 answered “unsafe” questions while improving its ability to answer benign ones.\n\nOpenAI announces new o3 reasoning models\n\nOpenAI CEO Sam Altman announced the successors to its o1 reasoning model family: o3 and o3-mini. The models are not widely available yet, but safety researchers can sign up for a preview. The reveal marks the end of the “12 Days of OpenAI” event, which saw announcements for real-time vision capabilities, ChatGPT Search, and even a Santa voice for ChatGPT.\n\nOpenAI brings ChatGPT to your landline\n\nIn an effort to make ChatGPT accessible to as many people as possible, OpenAI announced a 1-800 number to call the chatbot — even from a landline or a flip phone. Users can call 1-800-CHATGPT, and ChatGPT will respond to your queries in an experience that is more or less identical to Advanced Voice Mode — minus the multimodality.\n\nOpenAI is offering 15 minutes of free calling for U.S. users. The company notes that standard carrier fees may apply.\n\nOpenAI brings its ChatGPT Search to more users\n\nOpenAI is bringing ChatGPT Search to free, logged in users. Search gives ChatGPT the ability to access real-time information on the web to better answer your queries, but was only available for paid users when it launched in October. Not only is Search available now for free users, but it’s also been integrated into Advanced Voice Mode.\n\nOpenAI blames massive ChatGPT outage on a ‘new telemetry service’\n\nOpenAI is blaming one of the longest outages in its history on a “new telemetry service” gone awry. OpenAI wrote in a postmortem that the outage wasn’t caused by a security incident or recent product launch, but by a telemetry service it deployed to collect Kubernetes metrics.\n\nYou can make ChatGPT sound like Santa for a limited time\n\nOpenAI announced that ChatGPT users could access a new “Santa Mode” voice during December. The feature allows users to speak with ChatGPT’s Advanced Voice Mode, but with a Christmas twist. The voice sounds, well, “merry and bright,” as OpenAI described it. Think boomy, jolly — more or less like every Santa you’ve ever heard.\n\nOpenAI adds vision to Advanced Voice Mode\n\nOpenAI released the real-time video capabilities for ChatGPT that it demoed nearly seven months ago. ChatGPT Plus, Team, and Pro subscribers can use the app to point their phones at objects and have ChatGPT respond in near-real-time. The feature can also understand what’s on a device’s screen through screen sharing.\n\nThere’s more to come from OpenAI through December 23. Tune in to our live blog to stay updated.\n\nChatGPT and Sora hit with a major outage\n\nChatGPT and Sora both experienced a major outage Wednesday. Though users suspected the outage was due to the rollout of ChatGPT in Apple Intelligence, OpenAI developer community lead Edwin Arbus denied it in a post on X, saying the “outage was unrelated to 12 Days of OpenAI or Apple Intelligence. We made a config change that caused many servers to become unavailable.”\n\nChatGPT, API, and Sora were down today but we've recovered. https://t.co/OKiQYp3tXE — OpenAI (@OpenAI) December 12, 2024\n\nCanvas rolls out to everyone\n\nCanvas, OpenAI’s collaboration-focused interface for writing and code projects, is now rolling out to all users after being in beta for ChatGPT Plus members since October 2024. The company also announced the ability to integrate Python code within Canvas as well as bringing Canvas to custom GPTs.\n\nOpenAI pauses Sora sign-ups due to high demand\n\nOpenAI CEO Sam Altman posted on X that due to higher than expected demand, they are pausing new sign-ups for its video generator Sora and that video generations will be slower for the time being. The company released Sora as part of its “12 Days of OpenAI” event following nearly a year of teasing the product.\n\ndemand higher than expected; signups will be disabled on and off and generations will be slow for awhile.\n\n\n\ndoing our best! https://t.co/JU3WxE5bGl — Sam Altman (@sama) December 9, 2024\n\nOpenAI has finally released its text to video model, Sora. The model can generate videos up to 20 seconds long in 1080p based on text prompts or uploaded images, and can be “remixed” through additional user prompts. Sora is available starting today to ChatGPT Pro and Plus subscribers (except in the EU).\n\nIn Monday’s “12 Days of OpenAI” livestream, CEO Sam Altman said that ChatGPT Plus members will get 50 video generations a month, while ChatGPT Pro users will get “unlimited” generations in their “slow queue mode” and 500 “normal” generations per month.\n\nThere are still more reveals to come from OpenAI through December 23. Tune in to our live blog to stay updated.\n\nOpenAI launches $200 monthly ChatGPT Pro subscription — and full version of o1\n\nOn day one of its 12 Days of OpenAI event, the company announced a new — and expensive — subscription plan. ChatGPT Pro is a $200-per-month tier that provides unlimited access to all of OpenAI’s models, including the full version of its o1 “reasoning” model.\n\nThe full version of o1, which was released as a preview in September, can now reason about image uploads and has been trained to be “more concise in its thinking” to improve response times.\n\nOver the next few weeks, we’ll be updating all the news from OpenAI as it happens on our live blog. Follow along with us!\n\nOpenAI announces 12 days of reveals for the holidays\n\nOpenAI announced “12 Days of OpenAI,” which will feature livestreams every weekday starting December 5 at 10 a.m. PT. Each day’s stream is said to include either a product launch or a demo in varying sizes.\n\n🎄🎅starting tomorrow at 10 am pacific, we are doing 12 days of openai.\n\n\n\neach weekday, we will have a livestream with a launch or demo, some big ones and some stocking stuffers.\n\n\n\nwe’ve got some great stuff to share, hope you enjoy! merry christmas. — Sam Altman (@sama) December 4, 2024\n\nChatGPT surpasses 300M weekly active users, Sam Altman says\n\nAt the New York Times’ Dealbook Summit, OpenAI CEO Sam Altman said that ChatGPT has surpassed 300 million weekly active users. The milestone comes just a few months after the chatbot hit 200 million weekly active users in August 2024 and just over a year after reaching 100 million weekly active users in November 2023.\n\nNovember 2024\n\nUsers discovered the name ‘David Mayer’ crashed ChatGPT\n\nChatGPT users discovered an interesting phenomenon: the popular chatbot refused to answer questions asked about a “David Mayer,” and asking it to do so caused it to freeze up instantly. While the strange behavior spawned conspiracy theories, and a slew of other names being impacted, a much more ordinary reason may be at the heart of it: digital privacy requests.\n\nAds might be headed to ChatGPT\n\nOpenAI is toying with the idea of getting into ads. CFO Sarah Friar told the Financial Times it’s weighing an ads business model, with plans to be “thoughtful” about when and where ads appear — though she later stressed that the company has “no active plans to pursue advertising.” Still, the exploration may raise eyebrows given that Sam Altman recently said ads would be a “last resort.”\n\nCanadian news companies sue OpenAI\n\nA group of Canadian media companies, including the Toronto Star and the Globe and Mail, have filed a lawsuit against OpenAI. The companies behind the suit said that OpenAI infringed their copyrights and are seeking to win monetary damages — and ban OpenAI from making further use of their work.\n\nGPT-4o gets an upgrade\n\nOpenAI announced that its GPT-4o model has been updated to feature more “natural” and “engaging” creative writing abilities as well as more thorough responses and insights when accessing files uploaded by users.\n\nGPT-4o got an update 🎉\n\n\n\nThe model’s creative writing ability has leveled up–more natural, engaging, and tailored writing to improve relevance & readability.\n\n\n\nIt’s also better at working with uploaded files, providing deeper insights & more thorough responses. — OpenAI (@OpenAI) November 20, 2024\n\nOpenAI brings ChatGPT’s Advanced Voice Mode to the web\n\nChatGPT’s Advanced Voice Mode feature is expanding to the web, allowing users to talk to the chatbot through their browser. The conversational feature is rolling out to ChatGPT’s paying Plus, Enterprise, Teams, or Edu subscribers.\n\nRolling out to ChatGPT paid users this week: Advanced Voice Mode on web! 😍\n\n\n\nWe launched Advanced Voice Mode in our iOS and Android apps in September, and just recently brought them to our desktop apps (https://t.co/vVRYHXsbPD)—now we’re excited to add web to the mix. This means… pic.twitter.com/HtG5Km2OGh — Kevin Weil 🇺🇸 (@kevinweil) November 19, 2024\n\nChatGPT can now read some of your Mac’s desktop apps\n\nOpenAI announced the ChatGPT desktop app for macOS can now read code in a handful of developer-focused coding apps, such as VS Code, Xcode, TextEdit, Terminal, and iTerm2 — meaning that developers will no longer have to copy and paste their code into ChatGPT. When the feature is enabled, OpenAI will automatically send the section of code you’re working on through its chatbot as context, alongside your prompt.\n\nOpenAI loses another lead safety researcher\n\nLilian Weng announced on X that she is departing OpenAI. Weng served as VP of research and safety since August, and before that was the head of OpenAI’s safety systems team. It’s the latest in a long string of AI safety researchers,policy researchers, and other executives who have exited the company in the last year.\n\nAfter working at OpenAI for almost 7 years, I decide to leave. I learned so much and now I'm ready for a reset and something new.\n\n\n\nHere is the note I just shared with the team. 🩵 pic.twitter.com/2j9K3oBhPC — Lilian Weng (@lilianweng) November 8, 2024\n\nChatGPT told 2M people to get their election news elsewhere\n\nOpenAI stated that it told around 2 million users of ChatGPT to go elsewhere for information about the 2024 U.S. election, and instead recommended trusted news sources like Reuters and the Associated Press.\n\nIn a blog post, OpenAI said that ChatGPT sent roughly a million people to CanIVote.org when they asked questions specific to voting in the lead-up to the election and rejected around 250,000 requests to generate images of the candidates over the same period.\n\nOpenAI acquires Chat.com\n\nAdding to its collection of high-profile domain names, Chat.com now redirects to ChatGPT. Last year, it was reported that HubSpot co-founder and CTO Dharmesh Shah acquired Chat.com for $15.5 million, making it one of the top two all-time publicly reported domain sales — though OpenAI declined to state how much it paid for it.\n\nMeta’s former hardware lead for Orion is joining OpenAI\n\nThe former head of Meta’s augmented reality glasses efforts is joining OpenAI to lead robotics and consumer hardware. Kalinowski is a hardware executive who began leading Meta’s AR glasses team in March 2022. She oversaw the creation of Orion, the impressive augmented reality prototype that Meta recently showed off at its annual Connect conference.\n\nApple users will soon be able to upgrade to ChatGPT Plus in the Settings app\n\nApple is including an option to upgrade to ChatGPT Plus inside its Settings app, according to an update to the iOS 18.2 beta spotted by 9to5Mac. This will give Apple users a direct route to sign up for OpenAI’s premium subscription plan, which costs $20 a month.\n\nOctober 2024\n\nSam Altman says a lack of compute capacity is delaying product releases\n\nIn a Reddit AMA, OpenAI CEO Sam Altman admitted that a lack of compute capacity is one major factor preventing the company from shipping products as often as it’d like, including the vision capabilities for Advanced Voice Mode first teased in May. Altman also indicated that the next major release of DALL-E, OpenAI’s image generator, has no launch timeline, and that Sora, OpenAI’s video-generating tool, has also been held back.\n\nAltman also admitted to using ChatGPT “sometimes” to answer questions throughout the AMA.\n\nOpenAI launches its Google search challenger\n\nOpenAI launched ChatGPT Search, an evolution of the SearchGPT prototype it unveiled this summer. Powered by a fine-tuned version of OpenAI’s GPT-4o model, ChatGPT Search serves up information and photos from the web along with links to relevant sources, at which point you can ask follow-up questions to refine an ongoing search.\n\n🌐 Introducing ChatGPT search 🌐\n\n\n\nChatGPT can now search the web in a much better way than before so you get fast, timely answers with links to relevant web sources.https://t.co/7yilNgqH9T pic.twitter.com/z8mJWS8J9c — OpenAI (@OpenAI) October 31, 2024\n\nAdvanced Voice Mode comes to Mac and PC\n\nOpenAI has rolled out Advanced Voice Mode to ChatGPT’s desktop apps for macOS and Windows. For Mac users, that means that both ChatGPT’s Advanced Voice Mode can coexist with Siri on the same device, leading the way for ChatGPT’s Apple Intelligence integration.\n\nBig day for desktops.\n\n\n\nAdvanced Voice is now available in the macOS and Windows desktop apps.https://t.co/mv4ACwIhzA pic.twitter.com/HbwXbN9NkD — OpenAI (@OpenAI) October 30, 2024\n\nOpenAI is reportedly planning to build its first AI chip\n\nReuters reports that OpenAI is working with TSMC and Broadcom to build an in-house AI chip, which could arrive as soon as 2026. It appears, at least for now, the company has abandoned plans to establish a network of factories for chip manufacturing and is instead focusing on in-house chip design.\n\nYou can now search through your ChatGPT history\n\nOpenAI announced it’s rolling out a feature that allows users to search through their ChatGPT chat histories on the web. The new feature will let users bring up an old chat to remember something or pick back up a chat right where it was left off.\n\nWe’re starting to roll out the ability to search through your chat history on ChatGPT web.\n\n\n\nNow you can quickly & easily bring up a chat to reference, or pick up a chat where you left off. pic.twitter.com/YVAOUpFvzJ — OpenAI (@OpenAI) October 29, 2024\n\nWith the release of iOS 18.1, Apple Intelligence features powered by ChatGPT are now available to users. The ChatGPT features include integrated writing tools, image cleanup, article summaries, and a typing input for the redesigned Siri experience.\n\nOpenAI says it won’t release a model called Orion this year\n\nOpenAI denied reports that it is intending to release an AI model, code-named Orion, by December of this year. An OpenAI spokesperson told TechCrunch that they “don’t have plans to release a model code-named Orion this year,” but that leaves OpenAI substantial wiggle room.\n\nChatGPT comes to Windows\n\nOpenAI has begun previewing a dedicated Windows app for ChatGPT. The company says the app is an early version and is currently only available to ChatGPT Plus, Team, Enterprise, and Edu users with a “full experience” set to come later this year.\n\nOpenAI inks new content deal with Hearst\n\nOpenAI struck a content deal with Hearst, the newspaper and magazine publisher known for the San Francisco Chronicle, Esquire, Cosmopolitan, ELLE, and others. The partnership will allow OpenAI to surface stories from Hearst publications with citations and direct links.\n\nChatGPT has a new ‘Canvas’ interface for writing and coding projects\n\nOpenAI introduced a new way to interact with ChatGPT called “Canvas.” The canvas workspace allows for users to generate writing or code, then highlight sections of the work to have the model edit. Canvas is rolling out in beta to ChatGPT Plus and Teams, with a rollout to come to Enterprise and Edu tier users next week.\n\nWhen writing code, canvas makes it easier to track and understand ChatGPT’s changes.\n\n\n\nIt can also review code, add logs and comments, fix bugs, and port to other coding languages like JavaScript and Python. pic.twitter.com/Fxssd5pDl0 — OpenAI (@OpenAI) October 3, 2024\n\nOpenAI raises $6.6B and is now valued at $157B\n\nOpenAI has closed the largest VC round of all time. The startup announced it raised $6.6 billion in a funding round that values OpenAI at $157 billion post-money. Led by previous investor Thrive Capital, the new cash brings OpenAI’s total raised to $17.9 billion, per Crunchbase.\n\nDev Day brings Realtime API to AI app developers\n\nAt the first of its 2024 Dev Day events, OpenAI announced a new API tool that will let developers build nearly real-time, speech-to-speech experiences in their apps, with the choice of using six voices provided by OpenAI. These voices are distinct from those offered for ChatGPT, and developers can’t use third party voices, in order to prevent copyright issues.\n\nSeptember 2024\n\nOpenAI might raise the price of ChatGPT to $44 by 2029\n\nOpenAI is planning to raise the price of individual ChatGPT subscriptions from $20 per month to $22 per month by the end of the year, according to a report from The New York Times. The report notes that a steeper increase could come over the next five years; by 2029, OpenAI expects it’ll charge $44 per month for ChatGPT Plus.\n\nMira Murati exists OpenAI\n\nOpenAI CTO Mira Murati announced that she is leaving the company after more than six years. Hours after the announcement, OpenAI’s chief research officer, Bob McGrew, and a research VP, Barret Zoph, also left the company. CEO Sam Altman revealed the two latest resignations in a post on X, along with leadership transition plans.\n\ni just posted this note to openai:\n\n\n\nHi All–\n\n\n\nMira has been instrumental to OpenAI’s progress and growth the last 6.5 years; she has been a hugely significant factor in our development from an unknown research lab to an important company.\n\n\n\nWhen Mira informed me this morning that… — Sam Altman (@sama) September 26, 2024\n\nOpenAI rolls out Advanced Voice Mode with more voices and a new look\n\nAfter a delay, OpenAI is finally rolling out Advanced Voice Mode to an expanded set of ChatGPT’s paying customers. AVM is also getting a revamped design — the feature is now represented by a blue animated sphere instead of the animated black dots that were presented back in May. OpenAI is highlighting improvements in conversational speed, accents in foreign languages, and five new voices as part of the rollout.\n\nOpenAI is rolling out Advanced Voice Mode (AVM), an audio feature that makes ChatGPT more natural to speak with and includes five new voices pic.twitter.com/y97BCoob5b — TechCrunch (@TechCrunch) September 24, 2024\n\nYouTuber finds a way to run ChatGPT on a graphing calculator\n\nA video from YouTube creator ChromaLock showcased how to modify a TI-84 graphing calculator so that it can connect to the internet and access ChatGPT, touting it as the “ultimate cheating device.” As demonstrated in the video, it’s a pretty complicated process for the average high school student to follow — but it might stoke more concerns from teachers about the ongoing concerns about ChatGPT and cheating in schools.\n\nOpenAI announces OpenAI o1, a new model that can fact-check itself\n\nOpenAI unveiled a preview of OpenAI o1, also known as “Strawberry.” The collection of models are available in ChatGPT and via OpenAI’s API: o1-preview and o1 mini. The company claims that o1 can more effectively reason through math and science and fact-check itself by spending more time considering all parts of a command or question.\n\nUnlike ChatGPT, o1 can’t browse the web or analyze files yet, is rate-limited and expensive compared to other models. OpenAI says it plans to bring o1-mini access to all free users of ChatGPT, but hasn’t set a release date.\n\nOpenAI o1 codes a video game from a prompt. pic.twitter.com/aBEcehP0j8 — OpenAI (@OpenAI) September 12, 2024\n\nA hacker was able to trick ChatGPT into giving instructions on how to make bombs\n\nAn artist and hacker found a way to jailbreak ChatGPT to produce instructions for making powerful explosives, a request that the chatbot normally refuses. An explosives expert who reviewed the chatbot’s output told TechCrunch that the instructions could be used to make a detonatable product and was too sensitive to be released.\n\nOpenAI reaches 1 million paid users of its corporate offerings\n\nOpenAI announced it has surpassed 1 million paid users for its versions of ChatGPT intended for businesses, including ChatGPT Team, ChatGPT Enterprise and its educational offering, ChatGPT Edu. The company said that nearly half of OpenAI’s corporate users are based in the US.\n\nVolkswagen rolls out its ChatGPT assistant to the US\n\nVolkswagen is taking its ChatGPT voice assistant experiment to vehicles in the United States. Its ChatGPT-integrated Plus Speech voice assistant is an AI chatbot based on Cerence’s Chat Pro product and a LLM from OpenAI and will begin rolling out on September 6 with the 2025 Jetta and Jetta GLI models.\n\nAugust 2024\n\nOpenAI inks content deal with Condé Nast\n\nAs part of the new deal, OpenAI will surface stories from Condé Nast properties like The New Yorker, Vogue, Vanity Fair, Bon Appétit and Wired in ChatGPT and SearchGPT. Condé Nast CEO Roger Lynch implied that the “multi-year” deal will involve payment from OpenAI in some form and a Condé Nast spokesperson told TechCrunch that OpenAI will have permission to train on Condé Nast content.\n\nWe’re partnering with Condé Nast to deepen the integration of quality journalism into ChatGPT and our SearchGPT prototype. https://t.co/tiXqSOTNAl — OpenAI (@OpenAI) August 20, 2024\n\nOur first impressions of ChatGPT’s Advanced Voice Mode\n\nTechCrunch’s Maxwell Zeff has been playing around with OpenAI’s Advanced Voice Mode, in what he describes as “the most convincing taste I’ve had of an AI-powered future yet.” Compared to Siri or Alexa, Advanced Voice Mode stands out with faster response times, unique answers and the ability to answer complex questions. But the feature falls short as an effective replacement for virtual assistants.\n\nOpenAI shuts down election influence operation that used ChatGPT\n\nOpenAI has banned a cluster of ChatGPT accounts linked to an Iranian influence operation that was generating content about the U.S. presidential election. OpenAI identified five website fronts presenting as both progressive and conservative news outlets that used ChatGPT to draft several long-form articles, though it doesn’t seem that it reached much of an audience.\n\nOpenAI finds that GPT-4o does some weird stuff sometimes\n\nOpenAI has found that GPT-4o, which powers the recently launched alpha of Advanced Voice Mode in ChatGPT, can behave in strange ways. In a new “red teaming” report, OpenAI reveals some of GPT-4o’s weirder quirks, like mimicking the voice of the person speaking to it or randomly shouting in the middle of a conversation.\n\nChatGPT’s mobile app reports its biggest month yet\n\nAfter a big jump following the release of OpenAI’s new GPT-4o “omni” model, the mobile version of ChatGPT has now seen its biggest month of revenue yet. The app pulled in $28 million in net revenue from the App Store and Google Play in July, according to data provided by app intelligence firm Appfigures.\n\nOpenAI could potentially catch students who cheat with ChatGPT\n\nOpenAI has built a watermarking tool that could potentially catch students who cheat by using ChatGPT — but The Wall Street Journal reports that the company is debating whether to actually release it. An OpenAI spokesperson confirmed to TechCrunch that the company is researching tools that can detect writing from ChatGPT, but said it’s taking a “deliberate approach” to releasing it.\n\nJuly 2024\n\nChatGPT’s advanced Voice Mode starts rolling out to some users\n\nOpenAI is giving users their first access to GPT-4o’s updated realistic audio responses. The alpha version is now available to a small group of ChatGPT Plus users, and the company says the feature will gradually roll out to all Plus users in the fall of 2024. The release follows controversy surrounding the voice’s similarity to Scarlett Johansson, leading OpenAI to delay its release.\n\nWe’re starting to roll out advanced Voice Mode to a small group of ChatGPT Plus users. Advanced Voice Mode offers more natural, real-time conversations, allows you to interrupt anytime, and senses and responds to your emotions. pic.twitter.com/64O94EhhXK — OpenAI (@OpenAI) July 30, 2024\n\nOpenAI announces new search prototype, SearchGPT\n\nOpenAI is testing SearchGPT, a new AI search experience to compete with Google. SearchGPT aims to elevate search queries with “timely answers” from across the internet, as well as the ability to ask follow-up questions. The temporary prototype is currently only available to a small group of users and its publisher partners, like The Atlantic, for testing and feedback.\n\nWe’re testing SearchGPT, a temporary prototype of new AI search features that give you fast and timely answers with clear and relevant sources.\n\n\n\nWe’re launching with a small group of users for feedback and plan to integrate the experience into ChatGPT. https://t.co/dRRnxXVlGh pic.twitter.com/iQpADXmllH — OpenAI (@OpenAI) July 25, 2024\n\nOpenAI could lose $5 billion this year, report claims\n\nA new report from The Information, based on undisclosed financial information, claims OpenAI could lose up to $5 billion due to how costly the business is to operate. The report also says the company could spend as much as $7 billion in 2024 to train and operate ChatGPT.\n\nOpenAI unveils GPT-4o mini\n\nOpenAI released its latest small AI model, GPT-4o mini. The company says GPT-4o mini, which is cheaper and faster than OpenAI’s current AI models, outperforms industry leading small AI models on reasoning tasks involving text and vision. GPT-4o mini will replace GPT-3.5 Turbo as the smallest model OpenAI offers.\n\nOpenAI partners with Los Alamos National Laboratory for bioscience research\n\nOpenAI announced a partnership with the Los Alamos National Laboratory to study how AI can be employed by scientists in order to advance research in healthcare and bioscience. This follows other health-related research collaborations at OpenAI, including Moderna and Color Health.\n\nOpenAI and Los Alamos National Laboratory announce partnership to study AI for bioscience research https://t.co/WV4XMZsHBA — OpenAI (@OpenAI) July 10, 2024\n\nJune 2024\n\nOpenAI makes CriticGPT to find mistakes in GPT-4\n\nOpenAI announced it has trained a model off of GPT-4, dubbed CriticGPT, which aims to find errors in ChatGPT’s code output so they can make improvements and better help so-called human “AI trainers” rate the quality and accuracy of ChatGPT responses.\n\nWe’ve trained a model, CriticGPT, to catch bugs in GPT-4’s code. We’re starting to integrate such models into our RLHF alignment pipeline to help humans supervise AI on difficult tasks: https://t.co/5oQYfrpVBu — OpenAI (@OpenAI) June 27, 2024\n\nOpenAI inks content deal with TIME\n\nOpenAI and TIME announced a multi-year strategic partnership that brings the magazine’s content, both modern and archival, to ChatGPT. As part of the deal, TIME will also gain access to OpenAI’s technology in order to develop new audience-based products.\n\nWe’re partnering with TIME and its 101 years of archival content to enhance responses and provide links to stories on https://t.co/LgvmZUae9M: https://t.co/xHAYkYLxA9 — OpenAI (@OpenAI) June 27, 2024\n\nOpenAI delays ChatGPT’s new Voice Mode\n\nOpenAI planned to start rolling out its advanced Voice Mode feature to a small group of ChatGPT Plus users in late June, but it says lingering issues forced it to postpone the launch to July. OpenAI says Advanced Voice Mode might not launch for all ChatGPT Plus customers until the fall, depending on whether it meets certain internal safety and reliability checks.\n\nChatGPT releases app for Mac\n\nChatGPT for macOS is now available for all users. With the app, users can quickly call up ChatGPT by using the keyboard combination of Option + Space. The app allows users to upload files and other photos, as well as speak to ChatGPT from their desktop and search through their past conversations.\n\nThe ChatGPT desktop app for macOS is now available for all users.\n\n\n\nGet faster access to ChatGPT to chat about email, screenshots, and anything on your screen with the Option + Space shortcut: https://t.co/2rEx3PmMqg pic.twitter.com/x9sT8AnjDm — OpenAI (@OpenAI) June 25, 2024\n\nApple brings ChatGPT to its apps, including Siri\n\nApple announced at WWDC 2024 that it is bringing ChatGPT to Siri and other first-party apps and capabilities across its operating systems. The ChatGPT integrations, powered by GPT-4o, will arrive on iOS 18, iPadOS 18 and macOS Sequoia later this year, and will be free without the need to create a ChatGPT or OpenAI account. Features exclusive to paying ChatGPT users will also be available through Apple devices.\n\nApple is bringing ChatGPT to Siri and other first-party apps and capabilities across its operating systems #WWDC24\n\n\n\nRead more: https://t.co/0NJipSNJoS pic.twitter.com/EjQdPBuyy4 — TechCrunch (@TechCrunch) June 10, 2024\n\nHouse Oversight subcommittee invites Scarlett Johansson to testify about ‘Sky’ controversy\n\nScarlett Johansson has been invited to testify about the controversy surrounding OpenAI’s Sky voice at a hearing for the House Oversight Subcommittee on Cybersecurity, Information Technology, and Government Innovation. In a letter, Rep. Nancy Mace said Johansson’s testimony could “provide a platform” for concerns around deepfakes.\n\nChatGPT experiences two outages in a single day\n\nChatGPT was down twice in one day: one multi-hour outage in the early hours of the morning Tuesday and another outage later in the day that is still ongoing. Anthropic’s Claude and Perplexity also experienced some issues.\n\nYou're not alone, ChatGPT is down once again. pic.twitter.com/Ydk2vNOOK6 — TechCrunch (@TechCrunch) June 4, 2024\n\nMay 2024\n\nThe Atlantic and Vox Media ink content deals with OpenAI\n\nThe Atlantic and Vox Media have announced licensing and product partnerships with OpenAI. Both agreements allow OpenAI to use the publishers’ current content to generate responses in ChatGPT, which will feature citations to relevant articles. Vox Media says it will use OpenAI’s technology to build “audience-facing and internal applications,” while The Atlantic will build a new experimental product called Atlantic Labs.\n\nI am delighted that @theatlantic now has a strategic content & product partnership with @openai. Our stories will be discoverable in their new products and we'll be working with them to figure out new ways that AI can help serious, independent media : https://t.co/nfSVXW9KpB — nxthompson (@nxthompson) May 29, 2024\n\nOpenAI signs 100K PwC workers to ChatGPT’s enterprise tier\n\nOpenAI announced a new deal with management consulting giant PwC. The company will become OpenAI’s biggest customer to date, covering 100,000 users, and will become OpenAI’s first partner for selling its enterprise offerings to other businesses.\n\nOpenAI says it is training its GPT-4 successor\n\nOpenAI announced in a blog post that it has recently begun training its next flagship model to succeed GPT-4. The news came in an announcement of its new safety and security committee, which is responsible for informing safety and security decisions across OpenAI’s products.\n\nFormer OpenAI director claims the board found out about ChatGPT on Twitter\n\nOn the The TED AI Show podcast, former OpenAI board member Helen Toner revealed that the board did not know about ChatGPT until its launch in November 2022. Toner also said that Sam Altman gave the board inaccurate information about the safety processes the company had in place and that he didn’t disclose his involvement in the OpenAI Startup Fund.\n\nSharing this, recorded a few weeks ago. Most of the episode is about AI policy more broadly, but this was my first longform interview since the OpenAI investigation closed, so we also talked a bit about November.\n\n\n\nThanks to @bilawalsidhu for a fun conversation! https://t.co/h0PtK06T0K — Helen Toner (@hlntnr) May 28, 2024\n\nChatGPT’s mobile app revenue saw biggest spike yet following GPT-4o launch\n\nThe launch of GPT-4o has driven the company’s biggest-ever spike in revenue on mobile, despite the model being freely available on the web. Mobile users are being pushed to upgrade to its $19.99 monthly subscription, ChatGPT Plus, if they want to experiment with OpenAI’s most recent launch.\n\nOpenAI to remove ChatGPT’s Scarlett Johansson-like voice\n\nAfter demoing its new GPT-4o model last week, OpenAI announced it is pausing one of its voices, Sky, after users found that it sounded similar to Scarlett Johansson in “Her.”\n\nOpenAI explained in a blog post that Sky’s voice is “not an imitation” of the actress and that AI voices should not intentionally mimic the voice of a celebrity. The blog post went on to explain how the company chose its voices: Breeze, Cove, Ember, Juniper and Sky.\n\nWe’ve heard questions about how we chose the voices in ChatGPT, especially Sky. We are working to pause the use of Sky while we address them.\n\n\n\nRead more about how we chose these voices: https://t.co/R8wwZjU36L — OpenAI (@OpenAI) May 20, 2024\n\nChatGPT lets you add files from Google Drive and Microsoft OneDrive\n\nOpenAI announced new updates for easier data analysis within ChatGPT. Users can now upload files directly from Google Drive and Microsoft OneDrive, interact with tables and charts, and export customized charts for presentations. The company says these improvements will be added to GPT-4o in the coming weeks.\n\nWe're rolling out interactive tables and charts along with the ability to add files directly from Google Drive and Microsoft OneDrive into ChatGPT. Available to ChatGPT Plus, Team, and Enterprise users over the coming weeks. https://t.co/Fu2bgMChXt pic.twitter.com/M9AHLx5BKr — OpenAI (@OpenAI) May 16, 2024\n\nOpenAI inks deal to train AI on Reddit data\n\nOpenAI announced a partnership with Reddit that will give the company access to “real-time, structured and unique content” from the social network. Content from Reddit will be incorporated into ChatGPT, and the companies will work together to bring new AI-powered features to Reddit users and moderators.\n\nWe’re partnering with Reddit to bring its content to ChatGPT and new products: https://t.co/xHgBZ8ptOE — OpenAI (@OpenAI) May 16, 2024\n\nOpenAI debuts GPT-4o “omni” model now powering ChatGPT\n\nOpenAI’s spring update event saw the reveal of its new omni model, GPT-4o, which has a black hole-like interface, as well as voice and vision capabilities that feel eerily like something out of “Her.” GPT-4o is set to roll out “iteratively” across its developer and consumer-facing products over the next few weeks.\n\nOpenAI demos real-time language translation with its latest GPT-4o model. pic.twitter.com/pXtHQ9mKGc — TechCrunch (@TechCrunch) May 13, 2024\n\nOpenAI to build a tool that lets content creators opt out of AI training\n\nThe company announced it’s building a tool, Media Manager, that will allow creators to better control how their content is being used to train generative AI models — and give them an option to opt out. The goal is to have the new tool in place and ready to use by 2025.\n\nOpenAI explores allowing AI porn\n\nIn a new peek behind the curtain of its AI’s secret instructions, OpenAI also released a new NSFW policy. Though it’s intended to start a conversation about how it might allow explicit images and text in its AI products, it raises questions about whether OpenAI — or any generative AI vendor — can be trusted to handle sensitive content ethically.\n\nOpenAI and Stack Overflow announce partnership\n\nIn a new partnership, OpenAI will get access to developer platform Stack Overflow’s API and will get feedback from developers to improve the performance of their AI models. In return, OpenAI will include attributions to Stack Overflow in ChatGPT. However, the deal was not favorable to some Stack Overflow users — leading to some sabotaging their answer in protest.\n\nApril 2024\n\nU.S. newspapers file copyright lawsuit against OpenAI and Microsoft\n\nAlden Global Capital-owned newspapers, including the New York Daily News, the Chicago Tribune, and the Denver Post, are suing OpenAI and Microsoft for copyright infringement. The lawsuit alleges that the companies stole millions of copyrighted articles “without permission and without payment” to bolster ChatGPT and Copilot.\n\nOpenAI inks content licensing deal with Financial Times\n\nOpenAI has partnered with another news publisher in Europe, London’s Financial Times, that the company will be paying for content access. “Through the partnership, ChatGPT users will be able to see select attributed summaries, quotes and rich links to FT journalism in response to relevant queries,” the FT wrote in a press release.\n\nOpenAI opens Tokyo hub, adds GPT-4 model optimized for Japanese\n\nOpenAI is opening a new office in Tokyo and has plans for a GPT-4 model optimized specifically for the Japanese language. The move underscores how OpenAI will likely need to localize its technology to different languages as it expands.\n\nSam Altman pitches ChatGPT Enterprise to Fortune 500 companies\n\nAccording to Reuters, OpenAI’s Sam Altman hosted hundreds of executives from Fortune 500 companies across several cities in April, pitching versions of its AI services intended for corporate use.\n\nOpenAI releases “more direct, less verbose” version of GPT-4 Turbo\n\nPremium ChatGPT users — customers paying for ChatGPT Plus, Team or Enterprise — can now use an updated and enhanced version of GPT-4 Turbo. The new model brings with it improvements in writing, math, logical reasoning and coding, OpenAI claims, as well as a more up-to-date knowledge base.\n\nOur new GPT-4 Turbo is now available to paid ChatGPT users. We’ve improved capabilities in writing, math, logical reasoning, and coding.\n\nSource: https://t.co/fjoXDCOnPr pic.twitter.com/I4fg4aDq1T — OpenAI (@OpenAI) April 12, 2024\n\nChatGPT no longer requires an account — but there’s a catch\n\nYou can now use ChatGPT without signing up for an account, but it won’t be quite the same experience. You won’t be able to save or share chats, use custom instructions, or other features associated with a persistent account. This version of ChatGPT will have “slightly more restrictive content policies,” according to OpenAI. When TechCrunch asked for more details, however, the response was unclear:\n\n“The signed out experience will benefit from the existing safety mitigations that are already built into the model, such as refusing to generate harmful content. In addition to these existing mitigations, we are also implementing additional safeguards specifically designed to address other forms of content that may be inappropriate for a signed out experience,” a spokesperson said.\n\nMarch 2024\n\nOpenAI’s chatbot store is filling up with spam\n\nTechCrunch found that the OpenAI’s GPT Store is flooded with bizarre, potentially copyright-infringing GPTs. A cursory search pulls up GPTs that claim to generate art in the style of Disney and Marvel properties, but serve as little more than funnels to third-party paid services and advertise themselves as being able to bypass AI content detection tools.\n\nThe New York Times responds to OpenAI’s claims that it “hacked” ChatGPT for its copyright lawsuit\n\nIn a court filing opposing OpenAI’s motion to dismiss The New York Times’ lawsuit alleging copyright infringement, the newspaper asserted that “OpenAI’s attention-grabbing claim that The Times ‘hacked’ its products is as irrelevant as it is false.” The New York Times also claimed that some users of ChatGPT used the tool to bypass its paywalls.\n\nOpenAI VP doesn’t say whether artists should be paid for training data\n\nAt a SXSW 2024 panel, Peter Deng, OpenAI’s VP of consumer product dodged a question on whether artists whose work was used to train generative AI models should be compensated. While OpenAI lets artists “opt out” of and remove their work from the datasets that the company uses to train its image-generating models, some artists have described the tool as onerous.\n\nA new report estimates that ChatGPT uses more than half a million kilowatt-hours of electricity per day\n\nChatGPT’s environmental impact appears to be massive. According to a report from The New Yorker, ChatGPT uses an estimated 17,000 times the amount of electricity than the average U.S. household to respond to roughly 200 million requests each day.\n\nChatGPT can now read its answers aloud\n\nOpenAI released a new Read Aloud feature for the web version of ChatGPT as well as the iOS and Android apps. The feature allows ChatGPT to read its responses to queries in one of five voice options and can speak 37 languages, according to the company. Read aloud is available on both GPT-4 and GPT-3.5 models.\n\nChatGPT can now read responses to you. On iOS or Android, tap and hold the message and then tap “Read Aloud”. We’ve also started rolling on web – click the \"Read Aloud\" button below the message. pic.twitter.com/KevIkgAFbG — OpenAI (@OpenAI) March 4, 2024\n\nJanuary 2025\n\nOpenAI partners with Dublin City Council to use GPT-4 for tourism\n\nAs part of a new partnership with OpenAI, the Dublin City Council will use GPT-4 to craft personalized itineraries for travelers, including recommendations of unique and cultural destinations, in an effort to support tourism across Europe.\n\nA law firm used ChatGPT to justify a six-figure bill for legal services\n\nNew York-based law firm Cuddy Law was criticized by a judge for using ChatGPT to calculate their hourly billing rate. The firm submitted a $113,500 bill to the court, which was then halved by District Judge Paul Engelmayer, who called the figure “well above” reasonable demands.\n\nChatGPT experienced a bizarre bug for several hours\n\nChatGPT users found that ChatGPT was giving nonsensical answers for several hours, prompting OpenAI to investigate the issue. Incidents varied from repetitive phrases to confusing and incorrect answers to queries. The issue was resolved by OpenAI the following morning.\n\nMatch Group announced deal with OpenAI with a press release co-written by ChatGPT\n\nThe dating app giant home to Tinder, Match and OkCupid announced an enterprise agreement with OpenAI in an enthusiastic press release written with the help of ChatGPT. The AI tech will be used to help employees with work-related tasks and come as part of Match’s $20 million-plus bet on AI in 2024.\n\nChatGPT will now remember — and forget — things you tell it to\n\nAs part of a test, OpenAI began rolling out new “memory” controls for a small portion of ChatGPT free and paid users, with a broader rollout to follow. The controls let you tell ChatGPT explicitly to remember something, see what it remembers or turn off its memory altogether. Note that deleting a chat from chat history won’t erase ChatGPT’s or a custom GPT’s memories — you must delete the memory itself.\n\nWe’re testing ChatGPT's ability to remember things you discuss to make future chats more helpful. This feature is being rolled out to a small portion of Free and Plus users, and it's easy to turn on or off. https://t.co/1Tv355oa7V pic.twitter.com/BsFinBSTbs — OpenAI (@OpenAI) February 13, 2024\n\nOpenAI begins rolling out “Temporary Chat” feature\n\nInitially limited to a small subset of free and subscription users, Temporary Chat lets you have a dialogue with a blank slate. With Temporary Chat, ChatGPT won’t be aware of previous conversations or access memories but will follow custom instructions if they’re enabled.\n\nBut, OpenAI says it may keep a copy of Temporary Chat conversations for up to 30 days for “safety reasons.”\n\nUse temporary chat for conversations in which you don’t want to use memory or appear in history. pic.twitter.com/H1U82zoXyC — OpenAI (@OpenAI) February 13, 2024\n\nJanuary 2024\n\nChatGPT users can now invoke GPTs directly in chats\n\nPaid users of ChatGPT can now bring GPTs into a conversation by typing “@” and selecting a GPT from the list. The chosen GPT will have an understanding of the full conversation, and different GPTs can be “tagged in” for different use cases and needs.\n\nYou can now bring GPTs into any conversation in ChatGPT – simply type @ and select the GPT. This allows you to add relevant GPTs with the full context of the conversation. pic.twitter.com/Pjn5uIy9NF — OpenAI (@OpenAI) January 30, 2024\n\nChatGPT is reportedly leaking usernames and passwords from users’ private conversations\n\nScreenshots provided to Ars Technica found that ChatGPT is potentially leaking unpublished research papers, login credentials and private information from its users. An OpenAI representative told Ars Technica that the company was investigating the report.\n\nChatGPT is violating Europe’s privacy laws, Italian DPA tells OpenAI\n\nOpenAI has been told it’s suspected of violating European Union privacy, following a multi-month investigation of ChatGPT by Italy’s data protection authority. Details of the draft findings haven’t been disclosed, but in a response, OpenAI said: “We want our AI to learn about the world, not about private individuals.”\n\nOpenAI partners with Common Sense Media to collaborate on AI guidelines\n\nIn an effort to win the trust of parents and policymakers, OpenAI announced it’s partnering with Common Sense Media to collaborate on AI guidelines and education materials for parents, educators and young adults. The organization works to identify and minimize tech harms to young people and previously flagged ChatGPT as lacking in transparency and privacy.\n\nOpenAI responds to Congressional Black Caucus about lack of diversity on its board\n\nAfter a letter from the Congressional Black Caucus questioned the lack of diversity in OpenAI’s board, the company responded. The response, signed by CEO Sam Altman and Chairman of the Board Bret Taylor, said building a complete and diverse board was one of the company’s top priorities and that it was working with an executive search firm to assist it in finding talent.\n\nOpenAI drops prices and fixes ‘lazy’ GPT-4 that refused to work\n\nIn a blog post, OpenAI announced price drops for GPT-3.5’s API, with input prices dropping to 50% and output by 25%, to $0.0005 per thousand tokens in, and $0.0015 per thousand tokens out. GPT-4 Turbo also got a new preview model for API use, which includes an interesting fix that aims to reduce “laziness” that users have experienced.\n\nExpanding the platform for @OpenAIDevs: new generation of embedding models, updated GPT-4 Turbo, and lower pricing on GPT-3.5 Turbo. https://t.co/7wzCLwB1ax — OpenAI (@OpenAI) January 25, 2024\n\nOpenAI bans developer of a bot impersonating a presidential candidate\n\nOpenAI has suspended AI startup Delphi, which developed a bot impersonating Rep. Dean Phillips (D-Minn.) to help bolster his presidential campaign. The ban comes just weeks after OpenAI published a plan to combat election misinformation, which listed “chatbots impersonating candidates” as against its policy.\n\nOpenAI announces partnership with Arizona State University\n\nBeginning in February, Arizona State University will have full access to ChatGPT’s Enterprise tier, which the university plans to use to build a personalized AI tutor, develop AI avatars, bolster their prompt engineering course and more. It marks OpenAI’s first partnership with a higher education institution.\n\nWinner of a literary prize reveals around 5% her novel was written by ChatGPT\n\nAfter receiving the prestigious Akutagawa Prize for her novel The Tokyo Tower of Sympathy, author Rie Kudan admitted that around 5% of the book quoted ChatGPT-generated sentences “verbatim.” Interestingly enough, the novel revolves around a futuristic world with a pervasive presence of AI.\n\nSam Altman teases video capabilities for ChatGPT and the release of GPT-5\n\nIn a conversation with Bill Gates on the Unconfuse Me podcast, Sam Altman confirmed an upcoming release of GPT-5 that will be “fully multimodal with speech, image, code, and video support.” Altman said users can expect to see GPT-5 drop sometime in 2024.\n\nOpenAI announces team to build ‘crowdsourced’ governance ideas into its models\n\nOpenAI is forming a Collective Alignment team of researchers and engineers to create a system for collecting and “encoding” public input on its models’ behaviors into OpenAI products and services. This comes as a part of OpenAI’s public program to award grants to fund experiments in setting up a “democratic process” for determining the rules AI systems follow.\n\nOpenAI unveils plan to combat election misinformation\n\nIn a blog post, OpenAI announced users will not be allowed to build applications for political campaigning and lobbying until the company works out how effective their tools are for “personalized persuasion.”\n\nUsers will also be banned from creating chatbots that impersonate candidates or government institutions, and from using OpenAI tools to misrepresent the voting process or otherwise discourage voting.\n\nThe company is also testing out a tool that detects DALL-E generated images and will incorporate access to real-time news, with attribution, in ChatGPT.\n\nSnapshot of how we’re preparing for 2024’s worldwide elections: • Working to prevent abuse, including misleading deepfakes\n\n• Providing transparency on AI-generated content\n\n• Improving access to authoritative voting informationhttps://t.co/qsysYy5l0L — OpenAI (@OpenAI) January 15, 2024\n\nOpenAI changes policy to allow military applications\n\nIn an unannounced update to its usage policy, OpenAI removed language previously prohibiting the use of its products for the purposes of “military and warfare.” In an additional statement, OpenAI confirmed that the language was changed in order to accommodate military customers and projects that do not violate their ban on efforts to use their tools to “harm people, develop weapons, for communications surveillance, or to injure others or destroy property.”\n\nChatGPT subscription aimed at small teams debuts\n\nAptly called ChatGPT Team, the new plan provides a dedicated workspace for teams of up to 149 people using ChatGPT as well as admin tools for team management. In addition to gaining access to GPT-4, GPT-4 with Vision and DALL-E3, ChatGPT Team lets teams build and share GPTs for their business needs.\n\nOpenAI’s GPT store officially launches\n\nAfter some back and forth over the last few months, OpenAI’s GPT Store is finally here. The feature lives in a new tab in the ChatGPT web client, and includes a range of GPTs developed both by OpenAI’s partners and the wider dev community.\n\nTo access the GPT Store, users must be subscribed to one of OpenAI’s premium ChatGPT plans — ChatGPT Plus, ChatGPT Enterprise or the newly launched ChatGPT Team.\n\nthe GPT store is live!https://t.co/AKg1mjlvo2 fun speculation last night about which GPTs will be doing the best by the end of today. — Sam Altman (@sama) January 10, 2024\n\nDeveloping AI models would be “impossible” without copyrighted materials, OpenAI claims\n\nFollowing a proposed ban on using news publications and books to train AI chatbots in the U.K., OpenAI submitted a plea to the House of Lords communications and digital committee. OpenAI argued that it would be “impossible” to train AI models without using copyrighted materials, and that they believe copyright law “does not forbid training.”\n\nOpenAI claims The New York Times’ copyright lawsuit is without merit\n\nOpenAI published a public response to The New York Times’s lawsuit against them and Microsoft for allegedly violating copyright law, claiming that the case is without merit.\n\nIn the response, OpenAI reiterates its view that training AI models using publicly available data from the web is fair use. It also makes the case that regurgitation is less likely to occur with training data from a single source and places the onus on users to “act responsibly.”\n\nWe build AI to empower people, including journalists. Our position on the @nytimes lawsuit:\n\n• Training is fair use, but we provide an opt-out\n\n• \"Regurgitation\" is a rare bug we're driving to zero\n\n• The New York Times is not telling the full storyhttps://t.co/S6fSaDsfKb — OpenAI (@OpenAI) January 8, 2024\n\nOpenAI’s app store for GPTs planned to launch next week\n\nAfter being delayed in December, OpenAI plans to launch its GPT Store sometime in the coming week, according to an email viewed by TechCrunch. OpenAI says developers building GPTs will have to review the company’s updated usage policies and GPT brand guidelines to ensure their GPTs are compliant before they’re eligible for listing in the GPT Store. OpenAI’s update notably didn’t include any information on the expected monetization opportunities for developers listing their apps on the storefront.\n\nGPT Store launching next week – OpenAI pic.twitter.com/I6mkZKtgZG — Manish Singh (@refsrc) January 4, 2024\n\nOpenAI moves to shrink regulatory risk in EU around data privacy\n\nIn an email, OpenAI detailed an incoming update to its terms, including changing the OpenAI entity providing services to EEA and Swiss residents to OpenAI Ireland Limited. The move appears to be intended to shrink its regulatory risk in the European Union, where the company has been under scrutiny over ChatGPT’s impact on people’s privacy.\n\nFAQs:\n\nWhat is ChatGPT? How does it work?\n\nChatGPT is a general-purpose chatbot that uses artificial intelligence to generate text after a user enters a prompt, developed by tech startup OpenAI. The chatbot uses GPT-4, a large language model that uses deep learning to produce human-like text.\n\nWhen did ChatGPT get released?\n\nNovember 30, 2022 is when ChatGPT was released for public use.\n\nWhat is the latest version of ChatGPT?\n\nBoth the free version of ChatGPT and the paid ChatGPT Plus are regularly updated with new GPT models. The most recent model is GPT-4o.\n\nCan I use ChatGPT for free?\n\nThere is a free version of ChatGPT that only requires a sign-in in addition to the paid version, ChatGPT Plus.\n\nWho uses ChatGPT?\n\nAnyone can use ChatGPT! More and more tech companies and search engines are utilizing the chatbot to automate text or quickly answer user questions/concerns.\n\nWhat companies use ChatGPT?\n\nMultiple enterprises utilize ChatGPT, although others may limit the use of the AI-powered tool.\n\nMost recently, Microsoft announced at its 2023 Build conference that it is integrating it ChatGPT-based Bing experience into Windows 11. A Brooklyn-based 3D display startup Looking Glass utilizes ChatGPT to produce holograms you can communicate with by using ChatGPT. And nonprofit organization Solana officially integrated the chatbot into its network with a ChatGPT plug-in geared toward end users to help onboard into the web3 space.\n\nWhat does GPT mean in ChatGPT?\n\nGPT stands for Generative Pre-Trained Transformer.\n\nWhat is the difference between ChatGPT and a chatbot?\n\nA chatbot can be any software/system that holds dialogue with you/a person but doesn’t necessarily have to be AI-powered. For example, there are chatbots that are rules-based in the sense that they’ll give canned responses to questions.\n\nChatGPT is AI-powered and utilizes LLM technology to generate text after a prompt.\n\nCan ChatGPT write essays?\n\nYes.\n\nCan ChatGPT commit libel?\n\nDue to the nature of how these models work, they don’t know or care whether something is true, only that it looks true. That’s a problem when you’re using it to do your homework, sure, but when it accuses you of a crime you didn’t commit, that may well at this point be libel.\n\nWe will see how handling troubling statements produced by ChatGPT will play out over the next few months as tech and legal experts attempt to tackle the fastest moving target in the industry.\n\nDoes ChatGPT have an app?\n\nYes, there is a free ChatGPT mobile app for iOS and Android users.\n\nWhat is the ChatGPT character limit?\n\nIt’s not documented anywhere that ChatGPT has a character limit. However, users have noted that there are some character limitations after around 500 words.\n\nDoes ChatGPT have an API?\n\nYes, it was released March 1, 2023.\n\nWhat are some sample everyday uses for ChatGPT?\n\nEveryday examples include programming, scripts, email replies, listicles, blog ideas, summarization, etc.\n\nWhat are some advanced uses for ChatGPT?\n\nAdvanced use examples include debugging code, programming languages, scientific concepts, complex problem solving, etc.\n\nHow good is ChatGPT at writing code?\n\nIt depends on the nature of the program. While ChatGPT can write workable Python code, it can’t necessarily program an entire app’s worth of code. That’s because ChatGPT lacks context awareness — in other words, the generated code isn’t always appropriate for the specific context in which it’s being used.\n\nCan you save a ChatGPT chat?\n\nYes. OpenAI allows users to save chats in the ChatGPT interface, stored in the sidebar of the screen. There are no built-in sharing features yet.\n\nAre there alternatives to ChatGPT?\n\nYes. There are multiple AI-powered chatbot competitors such as Together, Google’s Gemini and Anthropic’s Claude, and developers are creating open source alternatives.\n\nHow does ChatGPT handle data privacy?\n\nOpenAI has said that individuals in “certain jurisdictions” (such as the EU) can object to the processing of their personal information by its AI models by filling out this form. This includes the ability to make requests for deletion of AI-generated references about you. Although OpenAI notes it may not grant every request since it must balance privacy requests against freedom of expression “in accordance with applicable laws”.\n\nThe web form for making a deletion of data about you request is entitled “OpenAI Personal Data Removal Request”.\n\nIn its privacy policy, the ChatGPT maker makes a passing acknowledgement of the objection requirements attached to relying on “legitimate interest” (LI), pointing users towards more information about requesting an opt out — when it writes: “See here for instructions on how you can opt out of our use of your information to train our models.”\n\nWhat controversies have surrounded ChatGPT?\n\nRecently, Discord announced that it had integrated OpenAI’s technology into its bot named Clyde where two users tricked Clyde into providing them with instructions for making the illegal drug methamphetamine (meth) and the incendiary mixture napalm.\n\nAn Australian mayor has publicly announced he may sue OpenAI for defamation due to ChatGPT’s false claims that he had served time in prison for bribery. This would be the first defamation lawsuit against the text-generating service.\n\nCNET found itself in the midst of controversy after Futurism reported the publication was publishing articles under a mysterious byline completely generated by AI. The private equity company that owns CNET, Red Ventures, was accused of using ChatGPT for SEO farming, even if the information was incorrect.\n\nSeveral major school systems and colleges, including New York City Public Schools, have banned ChatGPT from their networks and devices. They claim that the AI impedes the learning process by promoting plagiarism and misinformation, a claim that not every educator agrees with.\n\nThere have also been cases of ChatGPT accusing individuals of false crimes.\n\nWhere can I find examples of ChatGPT prompts?\n\nSeveral marketplaces host and provide ChatGPT prompts, either for free or for a nominal fee. One is PromptBase. Another is ChatX. More launch every day.\n\nCan ChatGPT be detected?\n\nPoorly. Several tools claim to detect ChatGPT-generated text, but in our tests, they’re inconsistent at best.\n\nAre ChatGPT chats public?\n\nNo. But OpenAI recently disclosed a bug, since fixed, that exposed the titles of some users’ conversations to other people on the service.\n\nWhat lawsuits are there surrounding ChatGPT?\n\nNone specifically targeting ChatGPT. But OpenAI is involved in at least one lawsuit that has implications for AI systems trained on publicly available data, which would touch on ChatGPT.\n\nAre there issues regarding plagiarism with ChatGPT?\n\nYes. Text-generating AI models like ChatGPT have a tendency to regurgitate content from their training data.",
      "url": "https://techcrunch.com/2025/03/21/chatgpt-everything-to-know-about-the-ai-chatbot/",
      "source": "https://techcrunch.com/category/artificial-intelligence/",
      "date": "2025-03-21",
      "authors": [
        "Kyle Wiggers",
        "Cody Corrall",
        "Alyssa Stringer",
        "Kate Park",
        "Ai Editor",
        "Audience Development Manager",
        "Reporter",
        "Natasha Lomas",
        "Charles Rollet",
        "Sean O'Kane"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "Yahoo Is Still Here—and It Has Big Plans for AI",
      "text": "In September 2021, Jim Lanzone took over a company whose name once embodied the go-go spirit of the internet but had, over the years, become a joke: Yahoo. He accepted the CEO post from the new private-equity owner Apollo Global Management, which had bought the property from Verizon, the most recent and possibly most clueless caretaker (high bar alert) in a long series of management shifts. Visiting him at the company’s offices in New York City, I ask him why he took the job. “I love turnarounds,” he says.\n\nThis is an essay from the latest edition of Steven Levy's Plaintext newsletter. SIGN UP for Plaintext to read the whole thing, and tap Steven's unique insights and unmatched contacts for the long view on tech.\n\nLanzone’s résumé confirms that. In 2001 he took over a sagging search property called AskJeeves—its share price was less than a dollar, down from a high of $196—and built it back to the point where Barry Diller’s IAC Corp bought it for $1.85 billion. At CBS Interactive and then CBS’s chief digital office during the 2010s, he yanked the stuffy Tiffany network into the streaming age. Yahoo, celebrating its 30th anniversary this month, might be his biggest challenge yet. Its history is pocked with missed opportunities, which explains in part why a public company once worth well over $100 billion was sold to a private equity firm for $5 billion in 2021. Yahoo famously passed on buying Google, and actually got Mark Zuckerberg to tentatively agree to sell Facebook for $1 billion before then CEO Terry Semel asked to renegotiate, which squelched the deal. Talent that walked out Yahoo’s door included the founders of WhatsApp. Promising acquisitions like Flickr, Tumblr and Huffington Post were ditched at fire-sale prices. In recent years Yahoo was a low-priority property for its owner, Verizon. Instead of trying to revive its purple glory, it merged Yahoo's assets with those of another failed icon, AOL, and dubbed the new brand Oath.\n\nSome pegged Lanzone’s chances at zero. “It’s hard to believe anyone else on the planet wants any part of his role, “ wrote George Bradt, one of those MBA types who churn out content for Forbes. Lanzone saw something different. In his view, Yahoo was an unacknowledged gem. “If you were able to take the name Yahoo off of it and look at the business in 2021, you saw billions in revenue,” he says.\n\nLanzone has little patience for exhuming past blunders. “I think the story of Yahoo's missed opportunities is tired,” he says. “It's boring.” Instead of crying over lost search glory, Lanzone concentrated on improving what Yahoo did. “We didn’t have to worry about what we weren’t,” he says. He got rid of money-losing units, like some nonperforming ad tech divisions, and quietly made some acquisitions to bolster the best properties, like Wagr, a sports betting app, to bring Yahoo Sports into the gambling age. He also brought in capable executives like former ESPN digital head Ryan Spoon, who now heads Yahoo Sports. He’s boosted profits and grown the company’s audience to the point where he says that Yahoo has performed the quickest return of any Apollo acquisition. Since Yahoo is private, the actual financials aren’t available. But Yahoo’s comms team provided me with a lengthy document packed with data to bolster Lanzone’s claim that Yahoo still has something to yodel about. Comscore, a marketing company that measures traffic, ranks Yahoo No. 1 in news, No. 1 in finance, and No. 3 in sports. It’s second only to Gmail in mail. He tells me that in the US alone, “hundreds of millions” of people use Yahoo every month.\n\nA year after Lanzone took the job, the entire tech world was turned around by the appearance of ChatGPT. In previous transformations like search, social, and mobile, Yahoo has a near-perfect record of botching these moments. Lanzone says Yahoo won’t be creating its own language models or dropping $100 billion on data centers, but he believes the company will seize the moment nonetheless. “I’d like to automate the word ‘AI’ so I don’t have to say it so much,” he says. Yahoo has in-house machine-learning talent and draws on outside companies for AI technology. For instance, it partners with the startup Sierra for robot customer service agents.\n\nOne of Lanzone’s canniest AI moves was acquiring Artifact, the AI-powered news aggregator created by Instagram cofounders Kevin Systrom and Mike Krieger. When the pair decided it would not become a viable business, they announced its closure and Lanzone was among multiple suitors vying for the underlying technology. It became the centerpiece of the homepage that Yahoo relaunched earlier this year. “Instead of incorporating their technology into our product, we did it the other way,” Lanzone says. “Essentially Yahoo News is now Artifact.” Systrom approves. “We partnered with Yahoo because they made a strong offer, but also because they planned on deploying our hard work to many millions of people,” he says.",
      "url": "https://www.wired.com/story/plaintext-yahoo-turns-30-jim-lanzone/",
      "source": "https://www.wired.com/tag/artificial-intelligence/",
      "date": "2025-03-21",
      "authors": [
        "Steven Levy",
        "Paresh Dave",
        "Will Knight",
        "Lauren Goode",
        "Sophie Charara",
        "Reece Rogers",
        "Makena Kelly"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "startup"
    },
    {
      "title": "Inside Google’s Two-Year Frenzy to Catch Up With OpenAI",
      "text": "But more piles of fascinating research are only useful to Google if they generate that most important of outputs: profit. Most customers generally aren’t yet willing to pay for AI features directly, so the company may be looking to sell ads in the Gemini app. That’s a classic strategy for Google, of course, one that long ago spread to the rest of Silicon Valley: Give us your data, your time, and your attention, check the box on our terms of service that releases us from liability, and we won’t charge you a dime for this cool tool we built.\n\nFor now, according to data from Sensor Tower, OpenAI’s estimated 600 million all-time global app installs for ChatGPT dwarf Google’s 140 million for the Gemini app. And there are plenty of other chatbots in this AI race too—Claude, Copilot, Grok, DeepSeek, Llama, Perplexity—many of them backed by Google’s biggest and best-funded competitors (or, in the case of Claude, Google itself). The entire industry, not just Google, struggles with the fact that generative AI systems have required billions of dollars in investment, so far unrecouped, and huge amounts of energy, enough to extend the lives of decades-old coal plants and nuclear reactors. Companies insist that efficiencies are adding up every day. They also hope to drive down errors to the point of winning over more users. But no one has truly figured out how to generate a reliable return or spare the climate.\n\nAnd Google faces one challenge that its competitors don’t: In the coming years, up to a quarter of its search ad revenue could be lost to antitrust judgments, according to JP Morgan analyst Doug Anmuth. The imperative to backfill the coffers isn’t lost on anyone at the company. Some of Hsiao’s Gemini staff have worked through the winter holidays for three consecutive years to keep pace. Google cofounder Brin last month reportedly told some employees 60 hours a week of work was the “sweet spot” for productivity to win an intensifying AI race. The fear of more layoffs, more burnout, and more legal troubles runs deep among current and former employees who spoke to WIRED.\n\nOne Google researcher and a high-ranking colleague say the pervasive feeling is unease. Generative AI clearly is helpful. Even governments that are prone to regulating big tech, such as France’s, are warming up to the technology’s lofty promises. Inside Google DeepMind and during public talks, Hassabis hasn’t relented an inch from his goal of creating artificial general intelligence, a system capable of human-level cognition across a range of tasks. He spends occasional weekends walking around London with his Astra prototype, getting a taste of a future in which the entire physical world, from that Thames duck over there to this Georgian manor over here, is searchable. But AGI will require systems to get better at reasoning, planning, and taking charge.\n\nIn January, OpenAI took a step toward that future by letting the public in on another experiment: its long-awaited Operator service, a so-called agentic AI that can act well beyond the chatbot window. Operator can click and type on websites just as a person would to execute chores like booking a trip or filling out a form. For the moment, it performs these tasks much more slowly and cautiously than a human would, and at a steep cost for its unreliability (available as part of a $200 monthly plan). Google, naturally, is working to bring agentic features to its coming models too. Where the current Gemini can help you develop a meal plan, the next one will place your ingredients in an online shopping cart. Maybe the one after that will give you real-time feedback on your onion-chopping technique.\n\nAs always, moving quickly may mean gaffing often. In late January, before the Super Bowl, Google released an ad in which Gemini was caught in a slipup even more laughably wrong than Bard’s telescope mistake: It estimated that half or more of all the cheese consumed on Earth is gouda. As Gemini grows from a sometimes-credible facts machine to an intimate part of human lives—life coach, all-seeing assistant—Pichai says that Google is proceeding cautiously. Back on top at last, though, he and the other Google executives may never want to get caught from behind again. The race goes on.\n\nUpdated 3/21/2025, 4 PM EDT: Wired has clarified the context of a quote attributed to Pandu Nayak.\n\nLet us know what you think about this article. Submit a letter to the editor at mail@wired.com.",
      "url": "https://www.wired.com/story/google-openai-gemini-chatgpt-artificial-intelligence/",
      "source": "https://www.wired.com/tag/artificial-intelligence/",
      "date": "2025-03-21",
      "authors": [
        "Paresh Dave",
        "Arielle Pardes",
        "Will Knight",
        "Makena Kelly",
        "Lauren Goode",
        "Reece Rogers",
        "Steven Levy",
        "Benj Edwards",
        "Ars Technica",
        "Jared Keller"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "research"
    },
    {
      "title": "'We Don’t Want an AI Demo, We Want Answers’: Federal Workers Grill Trump Appointee During All-Hands",
      "text": "On Thursday, Stephen Ehikian, the acting administrator of the General Services Administration, hosted his first all-hands meeting with GSA staff since his appointment to the position by President Donald Trump. The auditorium was packed, with hundreds of employees attending the meeting in person and thousands more tuning in online. While the tone of the live event remained polite, the chat that accompanied the live stream was a different story.\n\n“‘My door is always open’ but we’ve been told we can’t go to the floor you work on?” wrote one employee, according to Google Meet chat logs for the event obtained by WIRED. Employees used their real names to ask questions, but WIRED has chosen not to include those names to protect the privacy of the staffers. “We don’t want an AI demo, we want answers to what is going on with [reductions in force],” wrote another, as over 100 GSA staffers added a “thumbs up” emoji to the post.\n\nBut an AI demo is what they got. During the meeting, Ehikian and other high-ranking members of the GSA team showed off GSAi, a chatbot tool built by employees at the Technology Transformation Services. In its current form, the bot is meant to help employees with mundane tasks like writing emails. But Musk’s so-called Department of Government Efficiency (DOGE) has been pushing for a more complex version that could eventually tap into government databases. Roughly 1,500 people have access to GSAi today, and by tomorrow, the bot will be deployed to more than 13,000 GSA employees, WIRED has learned.\n\nMusk associates—including Ehikian and Thomas Shedd, a former Tesla engineer who now runs the Technology Transformation Services within GSA—have put AI at the heart of their agenda. Yesterday, GSA hosted a media roundtable to show its AI tool to reporters. “All information shared during this event is on deep background—attributable to a ‘GSA official familiar with the development of the AI tool,’” an invite read. (Reporters from Bloomberg, The Atlantic, and Fox were invited. WIRED was not.)\n\nGSA was one of the first federal agencies Musk’s allies took over in late January, WIRED reported. Ehikian, who is married to a former employee of Elon Musk’s X, works alongside Shedd and Nicole Hollander, who slept in Twitter HQ as an unofficial member of Musk’s transition team at the company. Hollander is partners with Steve Davis, who has taken a leading role at DOGE. More than 1,835 GSA employees have taken a deferred resignation offer since the leadership change, as DOGE continues its push to reportedly “right-size” the federal workforce. Employees who remain have been told to return to the office five days a week. Their credit cards—used for everything from paying for software tools to buying equipment for work—have a spending limit of $1.\n\nEmployees at the all-hands meeting—anxious to hear about whether more people will lose their jobs and why they’ve lost access to critical software tools—were not pleased. \"We are very busy after losing people and this is not [an] efficient use of time,” one employee wrote. “Literally who cares about this,” wrote another.",
      "url": "https://www.wired.com/story/gsa-staff-all-hands-meeting-ai/",
      "source": "https://www.wired.com/tag/artificial-intelligence/",
      "date": "2025-03-20",
      "authors": [
        "Zoë Schiffer",
        "Makena Kelly",
        "Will Knight",
        "Dell Cameron",
        "Jared Keller",
        "David Gilbert",
        "Reece Rogers",
        "Lauren Goode"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "Is That Painting a Lost Masterpiece or a Fraud? Let’s Ask AI",
      "text": "Artificial intelligence has to date been enlisted as a bogeyman in cultural circles: Software will take the jobs of writers and translators, and AI-generated images ring the death toll for illustrators and graphic designers.\n\nYet there’s a corner of high culture where AI is taking on a starring role as hero, not displacing the traditional protagonists—art experts and conservators—but adding a powerful, compelling weapon to their arsenal when it comes to fighting forgeries and misattributions. AI is already exceptionally good at recognizing and authenticating an artist’s work, based on the analysis of a digital image of a painting alone.\n\nAI’s objective analysis has thrown a wrench into this traditional hierarchy. If an algorithm can determine the authorship of an artwork with statistical probability, where does that leave the old-guard art historians whose reputations have been built on their subjective expertise? In truth, AI will never replace connoisseurs, just as the use of x-rays and carbon dating decades ago did not. It is simply the latest in a line of high-tech tools to assist with authentication.\n\nA good AI must be “fed” a curated dataset by human art historians to build up its knowledge of an artist’s style, and human art historians must interpret the results. Such was the case in November 2024, when a leading AI firm, Art Recognition, published its analysis of Rembrandt’s The Polish Rider—a painting that famously confounded scholars and led to many arguments as to how much, if any of it, had actually been painted by Rembrandt himself. The AI precisely matched what most connoisseurs had posited about which parts of the painting were by the master, which were by students of his, and which involved the hand of over-enthusiastic restorers. It is particularly compelling when the scientific approach confirms the expert opinion.\n\nWe humans find hard scientific data more compelling than personal opinion, even when that opinion comes from someone who seems to be an expert. The so-called “CSI effect” describes how jurors perceive DNA evidence as more persuasive than even eyewitness testimony. But when expert opinion (the eyewitnesses), provenance, and scientific tests (the CSI) all agree on the same conclusion? That’s as close to a definitive answer as one can get.\n\nBut what happens when the owner of a work that, at first glance, looks totally inauthentic to the point of being laughable, recruits a slick firm with the task of gathering forensic evidence to support a preferable attribution?\n\nLost and Found\n\nBack in 2016, an oil painting surfaced at a flea market in Minnesota and was bought for less than $50. Now its owners are suggesting that it could be a lost Van Gogh, and therefore would be worth millions. (One estimate suggests $15 million.) The answer—at least to anyone with functioning eyeballs and a passing familiarity with art history—was a resounding “nah.” The painting is stiff, clumsy, utterly lacking the feverish impasto and rhythmic brushwork that define the Dutch artist’s oeuvre. Worse still, it bore a signature: Elimar. And yet, this dubious painting has become the center of a high-stakes battle for authenticity, one in which scientific analysis, market forces, and wishful thinking collide.\n\nThe owners of the “Elimar Van Gogh,” as it has come to be derisively known in art circles, are now an art consultancy group called LMI International. They are investing heavily in getting experts to say what they want to hear: that it is, in fact, a genuine Van Gogh. This is where things get murky. The world of art authentication is not a straightforward affair. Unlike the hard sciences, art history deals in probabilities, connoisseurship, and competing expert opinions. It is also, crucially, an industry driven by financial incentives. If the painting is deemed real, its value skyrockets. If it’s deemed a fake, or rather in this case a derivative work by someone named Elimar who daubed a bit on canvas, distantly inspired by Van Gogh perhaps, but with none of his talents, it's virtually worthless—about as valuable as you might expect to find at a flea market in Minnesota for under 50 bucks. This imbalance in stakes has led to a dangerous trend: hiring experts not to determine authenticity, but to affirm it.",
      "url": "https://www.wired.com/story/is-that-painting-a-lost-masterpiece-or-a-fraud-lets-ask-ai/",
      "source": "https://www.wired.com/tag/artificial-intelligence/",
      "date": "2025-03-20",
      "authors": [
        "Noah Charney",
        "David Gilbert",
        "Will Knight",
        "Emily Mullin",
        "Steven Levy",
        "Nicholas Lalla",
        "Carlton Reid",
        "Benj Edwards",
        "Ars Technica",
        "Makena Kelly"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "startup"
    },
    {
      "title": "Satellite Internet Will Enable AI in Everything",
      "text": "Satellite internet is blasting off right now.\n\nNations and states are inking deals with satellite providers to fill in service gaps for their residents and keep their critical infrastructure connected. Phone makers are building satellite capabilities into their handsets. Airlines are partnering with satellite operators to keep your in-flight Netflix stream stutter-free. And the race to blast the satellites powering these networks into orbit is helping the rocket business thrive.\n\nAll of this adds up to boom times for satellite internet. But there’s another factor that could cause the tech’s proliferation to accelerate further: artificial intelligence.\n\nThe AI industry is keen to see a fully connected world because of the benefits a persistent connection can bring to its products, says Anshel Sag, principal analyst at Moor Insights & Strategy. The near future of the AI arms race hinges on agents, smart-ish virtual assistants that can automate various parts of your life. But those AI agents have to be on call 24/7 to be effective, which requires an always-on internet connection. And the fast pace of AI agent innovation requires the AI models to be tweaked and updated often, which makes a direct connection indispensable.\n\n“We're still very much dependent on the cloud because things are changing so fast,” Sag says. “You can't just deploy an AI model to an endpoint and expect that you're not going to have to update that model pretty regularly.”\n\nAnother beneficiary of the satellite internet expansion is likely to be the great, flawed landscape that is the internet of things. IoT tech like free-roaming robot vacuums, road-tripping luggage trackers, and security cameras at the edge of your property will no longer struggle to stay connected while they transfer videos, photos, commands, and location data.\n\n“I think IoT will become more relevant,” Sag says, “because satellite connectivity will enable more IoT devices to feed back into AI.”\n\nIf you have satellites blanketing the entire planet giving real-time data of where devices are and how they’re moving, that offers up a massive feast of information for AI to gobble up and, hopefully, to digest into something usable.\n\n“IoT struggled significantly because nobody knew what to do with the data,” Sag says of the past decade. “But AI loves data. And the more data you give it, the more you can empower it to make better decisions.”\n\nA Link to the Stars\n\nOf all the major players in satellite internet, Starlink looms the largest. It provides solid internet service to over 4.5 million subscribers around the world, many of whom would otherwise not have access to a reliable connection. It’s also a subsidiary of SpaceX and controlled by CEO Elon Musk, the person leading the systematic dismemberment of federal agencies across the entire pantheon of government in the US. Thanks to Musk, Starlink’s internet service is even installed at the White House. That political connection is unsavory to some, and it’s sending some potential customers elsewhere. Starlink is controversial for other reasons too: The service was found to be used by a criminal organization in Myanmar to keep a slavery-powerd scam operation online.\n\nBut Starlink is beloved by millions of rural residents and rich yacht owners alike. The US military is excited about using it to keep troops connected in the field. Corporations are hitching their wagon to Starlink as well, with travel providers like United Airlines and cruise ship companies hoping to keep customers online as they scurry around the globe.",
      "url": "https://www.wired.com/story/satellite-internet-will-let-us-put-ai-in-everything/",
      "source": "https://www.wired.com/tag/artificial-intelligence/",
      "date": "2025-03-20",
      "authors": [
        "Boone Ashworth",
        "Lily Hay Newman",
        "Will Knight",
        "Paresh Dave",
        "Zeyi Yang",
        "Lauren Goode",
        "Brian Barrett",
        "Makena Kelly",
        "Reece Rogers"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "news"
    },
    {
      "title": "How to pitch MIT Technology Review",
      "text": "MIT Technology Review welcomes pitches from freelance writers across a broad range of topics. We encourage writers of all skill levels, and from all backgrounds, to pitch us. We are especially interested in hearing from writers with backgrounds that are traditionally under-represented in technology and science journalism, and from outside the United States.\n\nThis guide will give you a brief overview of the kinds of stories we run, what we’re looking for in a pitch, and who to contact when you have an idea.\n\nMost of the stories that MIT Technology Review commissions come from pitches, but we do reach out to writers with assignments. We also issue a bi-monthly call for pitches for the print magazine. If you’d like to be considered for assignments and be informed of calls for pitches, you can fill out this form to be added to our database of freelance journalists.\n\nWhat is an MIT Technology Review story?\n\nMIT Technology Review publishes a wide range of stories, from breaking news to long-term investigations. We want stories about technology itself, but we’re particularly excited about those areas where technology and humans collide—where technology meets the real world. Our stories should feel like must-reads for people who want to stay up to date with how technology is changing our world.\n\nIn general, we commission news stories (generally 800-1,000 words), analysis pieces (800-1,000 words), and features (2,500-4,000 words) from freelancers for the website. The print magazine offers other potential formats for stories in addition to news and features, including infographics and data spreads, essays, short profiles, and book reviews. If you are an expert interested in writing an op-ed for the magazine, you can find more information on the op-ed pitching and writing process here.\n\nWhat makes a good MIT Technology Review pitch?\n\nA pitch does not need to be long, but it should contain enough information to give the commissioning editor a good sense of what the end product would look like. A good pitch will tell us why our readers should care about your idea and why they should be reading about it now. It should also show how you (the writer) will make the story an enjoyable read.\n\nIn general, there has to be a sense of urgency and a clear top line. Pitching to write about a topic isn’t enough: What is the story you’re going to tell us? What kind of take-away should readers have? One way to do this is to think about what the headline might be. You can look at our site to get a sense of how we write our headlines (for example, we generally avoid formulating headlines as questions).\n\nPlease check to see if we have published similar stories recently. If it is an idea that hasn’t been covered by us but has been covered well by other outlets, explain how your story would stand apart.\n\nAnd if you have not written for MIT Technology Review before, be sure to introduce yourself and include links to a few relevant clips if you have them.\n\nNews and analysis stories\n\nIf you’re pitching a news story about a scientific or technological development, be sure to make clear why this advance (if it is an advance) is important and not just incremental. It can’t be significant only to researchers in the field: it has to be important to people in general. They should be able to read the story and instantly get a sense as to why it’s a big deal.\n\nIn addition to straight news stories, we sometimes commission analysis pieces from freelance writers (examples here, here and here) that look at the news of the day through a new lens, adding context, insight, or new information to take the story forward.\n\nWe’d want a quick pitch (and a quick turnaround) for such analysis pieces. Summing up what is already out there isn’t enough; you need to tell us what the new thing is you’re bringing to the story. What do people not know? What are the consequences of the event that many of our readers will be unaware of—but would be fascinated to learn about?\n\nFeatures and essays\n\nFeatures can take a wide range of possible approaches, including profiles, narrative features, and deep investigations. Ultimately, readers should enjoy reading them. So your pitch should not only explain what the story is and why it is important but show how you’d tell it in a compelling way.\n\nFeature pitches should have a specific story to tell—it is not enough to pitch an interesting topic. You must explain what kind of approach you’d take, what kind of ride you’ll take readers on. Who are the main characters? What obstacles or challenges will they meet on the way? How might your story resolve? What is the nut graf? We don’t expect you to have everything worked out in advance of reporting and writing the story, but the pitch should be able to give a sense of where the story will go.\n\nSimilarly, pitches for essays need a clear argument. Explain what you want to say about the topic in hand, why you're qualified to write about it, what take-aways you hope to give readers, and the evidence you will marshal to make your case.\n\nPitching for print\n\nMIT Technology Review publishes a bi-monthly print magazine. We run short news stories and profiles (500-800 words), op-eds, and data spreads in the front of the book and essays and book reviews in the back of the book (usually around 2000 words).\n\nThe feature well of the magazine is devoted to narrative features, investigations, big profiles, and reported essays (generally between 2500-4000 words). Our features in each issue are centered around a theme.\n\nHere are the upcoming themes for 2025. Please note that specific pitch deadlines will be announced over e-mail. You can add yourself to the freelance database through the form linked at the top of this page to get the official call for pitches for each issue:\n\nJan/Feb: Breakthroughs [includes our annual round-up of high-impact innovations, pitches due late July/early August 2024]\n\n[includes our annual round-up of high-impact innovations, pitches due late July/early August 2024] March/April: Relationships [pitches due mid Sept 2024]\n\n[pitches due mid Sept 2024] May/June: Creativity [pitches due mid-to-late November 2024]\n\n[pitches due mid-to-late November 2024] July/August: Power [pitches due mid-to-late Jan 2025]\n\n[pitches due mid-to-late Jan 2025] Sept/Oct: Security [pitches due mid-to-late March 2025]\n\n[pitches due mid-to-late March 2025] Nov/Dec: The Body [pitches due mid-to-late May 2025]\n\nHow much does MIT Technology Review pay?\n\nRates range from $1 to $2 per word, depending on the experience level of the writer, the story, and the publication route. Deeply reported features pay more than shorter news pieces.\n\nWho do I pitch to?\n\nIf you have any questions or if you are ready to send a pitch, please reach out to commissioning editor Rachel Courtland (rachel.courtland@technologyreview.com). Please be sure to include “PITCH” in the subject line of your email. We prefer pitches to be within the body of an e-mail instead of as an attachment.",
      "url": "https://www.technologyreview.com/how-to-pitch-mit-technology-review/",
      "source": "https://www.technologyreview.com/topic/artificial-intelligence/",
      "date": null,
      "authors": [],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "Inside a new quest to save the “doomsday glacier”",
      "text": "That would mark the start of a global climate disaster. The glacier itself holds enough ice to raise ocean levels by more than two feet, which could flood coastlines and force tens of millions of people living in low-lying areas to abandon their homes.\n\nThe loss of the entire ice sheet—which could still take centuries to unfold—would push up sea levels by 11 feet and redraw the contours of the continents.\n\nThis is why Thwaites is known as the doomsday glacier—and why scientists are eager to understand just how likely such a collapse is, when it could happen, and if we have the power to stop it.\n\nScientists at MIT and Dartmouth College founded Arête Glacier Initiative last year in the hope of providing clearer answers to these questions. The nonprofit research organization will officially unveil itself, launch its website, and post requests for research proposals today, March 21, timed to coincide with the UN’s inaugural World Day for Glaciers, MIT Technology Review can report exclusively.\n\nArête will also announce it is issuing its first grants, each for around $200,000 over two years, to a pair of glacier researchers at the University of Wisconsin-Madison.\n\nOne of the organization’s main goals is to study the possibility of preventing the loss of giant glaciers, Thwaites in particular, by refreezing them to the bedrock. It would represent a radical intervention into the natural world, requiring a massive, expensive engineering project in a remote, treacherous environment.\n\nBut the hope is that such a mega-adaptation project could minimize the mass relocation of climate refugees, prevent much of the suffering and violence that would almost certainly accompany it, and help nations preserve trillions of dollars invested in high-rises, roads, homes, ports, and airports around the globe.\n\n“About a million people are displaced per centimeter of sea-level rise,” says Brent Minchew, an associate professor of geophysics at MIT, who cofounded Arête Glacier Initiative and will serve as its chief scientist. “If we’re able to bring that down, even by a few centimeters, then we would safeguard the homes of millions.”",
      "url": "https://www.technologyreview.com/2025/03/21/1113396/inside-a-new-quest-to-save-the-doomsday-glacier/",
      "source": "https://www.technologyreview.com/topic/artificial-intelligence/",
      "date": "2025-03-21",
      "authors": [
        "James Temple"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "OpenAI has released its first research into how using ChatGPT affects people’s emotional wellbeing",
      "text": "The researchers found some intriguing differences between how men and women respond to using ChatGPT. After using the chatbot for four weeks, female study participants were slightly less likely to socialize with people than their male counterparts who did the same. Meanwhile, participants who interacted with ChatGPT’s voice mode in a gender that was not their own for their interactions reported significantly higher levels of loneliness and more emotional dependency on the chatbot at the end of the experiment. OpenAI plans to submit both studies to peer-reviewed journals.\n\n\n\nChatbots powered by large language models are still a nascent technology, and it’s difficult to study how they affect us emotionally. A lot of existing research in the area—including some of the new work by OpenAI and MIT—relies upon self-reported data, which may not always be accurate or reliable. That said, this latest research does chime with what scientists so far have discovered about how emotionally compelling chatbot conversations can be. For example, in 2023 MIT Media Lab researchers found that chatbots tend to mirror the emotional sentiment of a user’s messages, suggesting a kind of feedback loop where the happier you act, the happier the AI seems, or on the flipside, if you act sadder, so does the AI.\n\nOpenAI and the MIT Media Lab used a two-pronged method. First they collected and analyzed real-world data from close to 40 million interactions with ChatGPT. Then they asked the 4,076 users who’d had those interactions how they made them feel. Next, the Media Lab recruited almost 1,000 people to take part in a four-week trial. This was more in-depth, examining how participants interacted with ChatGPT for a minimum of five minutes each day. At the end of the experiment, participants completed a questionnaire to measure their perceptions of the chatbot, their subjective feelings of loneliness, their levels of social engagement, their emotional dependence on the bot, and their sense of whether their use of the bot was problematic. They found that participants who trusted and “bonded” with ChatGPT more were likelier than others to be lonely, and to rely on it more.\n\nThis work is an important first step toward greater insight into ChatGPT’s impact on us, which could help AI platforms enable safer and healthier interactions, says Jason Phang, an OpenAI safety researcher who worked on the project.\n\n“A lot of what we’re doing here is preliminary, but we’re trying to start the conversation with the field about the kinds of things that we can start to measure, and to start thinking about what the long-term impact on users is,” he says.\n\nAlthough the research is welcome, it’s still difficult to identify when a human is—and isn’t—engaging with technology on an emotional level, says Devlin. She says the study participants may have been experiencing emotions that weren’t recorded by the researchers.\n\n“In terms of what the teams set out to measure, people might not necessarily have been using ChatGPT in an emotional way, but you can’t divorce being a human from your interactions [with technology],” she says. “We use these emotion classifiers that we have created to look for certain things—but what that actually means to someone’s life is really hard to extrapolate.\"\n\nCorrection: An earlier version of this article misstated that study participants set the gender of ChatGPT's voice, and that OpenAI did not plan to publish either study. Study participants were assigned the voice mode gender, and OpenAI plans to submit both studies to peer-reviewed journals. The article has since been updated.",
      "url": "https://www.technologyreview.com/2025/03/21/1113635/openai-has-released-its-first-research-into-how-using-chatgpt-affects-peoples-emotional-wellbeing/",
      "source": "https://www.technologyreview.com/topic/artificial-intelligence/",
      "date": "2025-03-21",
      "authors": [
        "Rhiannon Williams"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "research"
    },
    {
      "title": "4 technologies that could power the future of energy",
      "text": "Hundreds of projects were exhibited in a massive hall during the conference, featuring demonstrations and research results. Here are four of the most interesting innovations MIT Technology Review spotted on site.\n\nSteel made with lasers\n\nStartup Limelight Steel has developed a process to make iron, the main component in steel, by using lasers to heat iron ore to super-high temperatures.\n\nSteel production makes up roughly 8% of global greenhouse gas emissions today, in part because most steel is still made with blast furnaces, which rely on coal to hit the high temperatures that kick off the required chemical reactions.\n\nLimelight instead shines lasers on iron ore, heating it to temperatures over 1,600 °C. Molten iron can then be separated from impurities, and the iron can be put through existing processes to make steel.\n\nThe company has built a small demonstration system with a laser power of about 1.5 kilowatts, which can process between 10 and 20 grams of ore. The whole system is made up of 16 laser arrays, each just a bit larger than a postage stamp.\n\nThe components in the demonstration system are commercially available; this particular type of laser is used in projectors. The startup has benefited from years of progress in the telecommunications industry that has helped bring down the cost of lasers, says Andy Zhao, the company’s cofounder and CTO.\n\nThe next step is to build a larger-scale system that will use 150 kilowatts of laser power and could make up to 100 tons of steel over the course of a year.\n\nRocks that can make fuel\n\nThe hunks of rock at a booth hosted by MIT might not seem all that high-tech, but someday they could help produce fuels and chemicals.",
      "url": "https://www.technologyreview.com/2025/03/19/1113381/energy-technology-lasers-steel-batteries/",
      "source": "https://www.technologyreview.com/topic/artificial-intelligence/",
      "date": "2025-03-19",
      "authors": [
        "Casey Crownhart"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    },
    {
      "title": "HIV could infect 1,400 infants every day because of US aid disruptions",
      "text": "Rubio approved a waiver for “life-saving” humanitarian assistance on January 28. “This resumption is temporary in nature, and with limited exceptions as needed to continue life-saving humanitarian assistance programs, no new contracts shall be entered into,” he said in a statement at the time.\n\nThe US President’s Emergency Plan for AIDS Relief (PEPFAR), which invests millions of dollars in the global AIDS response every year, was also granted a waiver February 1 to continue “life-saving” work.\n\nDespite this waiver, there have been devastating reports of the impact on health programs across the many low-income countries that relied on the US Agency for International Development (USAID), which oversees PEPFAR, for funding. To get a better sense of the overall impact, amfAR conducted two surveys looking at more than 150 organizations that rely on PEPFAR funding in more than 26 countries.\n\n“We found really severe disruptions to HIV services,” said Sherwood, who presented the findings at Columbia. “About 90% of our participants said [the cuts] had severely limited their ability to deliver HIV services.” Specifically, 94% of follow-up services designed to monitor people’s progress were either canceled or disrupted. There were similarly dramatic disruptions to services for HIV testing, treatment, and prevention, and 92% of services for gender-based violence were canceled or disrupted.\n\nThe cuts have plunged organizations into a “deep financial crisis,” said Sherwood. Almost two-thirds of respondents said community-based staff were laid off before the end of January. When the team asked these organizations how long they could stay open without US funding, 36% said they had already closed. “Only 14% said that they were able to stay open longer than a month,” said Sherwood. “And … this data was collected longer than a month ago.”\n\nThe organizations said tens of thousands of the people they serve would lose HIV treatment within a month. For some organizations, that figure was over 100,000, said Sherwood.",
      "url": "https://www.technologyreview.com/2025/03/18/1113288/hiv-could-infect-1400-infants-every-day-due-to-disruptions-in-aid-from-the-us/",
      "source": "https://www.technologyreview.com/topic/artificial-intelligence/",
      "date": "2025-03-18",
      "authors": [
        "Jessica Hamzelou"
      ],
      "summary": "",
      "scrape_method": "newspaper3k",
      "category": "event"
    }
  ]
}